[{"path":"index.html","id":"introduction-to-the-practical","chapter":"Introduction to the Practical","heading":"Introduction to the Practical","text":"website find resources practical session module Essentials Mathematics Statistics (EMS) part MSc Bioinformatics course University Birmingham.","code":""},{"path":"index.html","id":"what-is-this-module-about","chapter":"Introduction to the Practical","heading":"What is this module about?","text":"module covers basics mathematics statistics need understand rest modules course. important good understanding concepts used modules. resource designed help understand concepts covered module applying real simulated data. essential part learning help understand concepts better.","code":""},{"path":"index.html","id":"what-will-you-learn","chapter":"Introduction to the Practical","heading":"What will you learn?","text":"topics covered practical include:Sampling random variableProbability RSimulating Markov ChainsMonte Carlo SimulationHypothesis TestingLinear regressionPrincipal Component Analysis (PCA)Multivariate RegressionGeneralised Linear Models","code":""},{"path":"index.html","id":"how-to-use-these-resources","chapter":"Introduction to the Practical","heading":"How to use these resources","text":"basic concepts familiar starting practical. familiar concepts, review starting practical. can find resources concepts Introduction R section.find information accessing data practical sessions Data section.practical sessions divided sections, covering different topic. Broadly fall two categories first part covers probability R second part covers Statistical Modelling.\ncan navigate sections using links navigation bar. section contains brief introduction topic, followed series exercises. complete exercises order, following lectures introduce concepts.ensure attend practical sessions get resource. practical sessions provide opportunity ask questions, get help exercises, go details.session contain number exercises complete moving next session. can check answers comparing solutions provided resource.","code":""},{"path":"start.html","id":"start","chapter":"Getting started in R and Rstudio","heading":"Getting started in R and Rstudio","text":"already started working R, just core principles revisited. Ensure understand required knowledge covered module.","code":""},{"path":"start.html","id":"r-scripts","chapter":"Getting started in R and Rstudio","heading":"R scripts","text":"entering running code R command line effective simple. technique limitations. time want execute set commands, re-enter command line. Complex commands potentially subject typographical errors, necessitating re-entered correctly. Repeating set operations requires re-entering code stream. Fortunately, R RStudio provide method mitigate issues. R scripts solution. script simply text file containing set commands comments. script can saved used later re-execute saved commands. script can also edited can execute modified version commands.","code":""},{"path":"start.html","id":"creating-an-r-script","chapter":"Getting started in R and Rstudio","heading":"Creating an R script","text":"easy create new script RStudio. can open new empty script clicking New File icon upper left main RStudio toolbar. icon looks like white square white plus sign green circle. Clicking icon opens New File Menu. Click R Script menu option script editor open empty script.new script opens Script Editor panel, script ready text entry, RStudio session look like .easy example familiarize Script Editor interface. Type following code new script (later topics explain specific code components )., now first R script. Notice editor places number front line code. line numbers can helpful work code. proceeding executing code, good idea learn save script.","code":"# this is my first R script\n# do some things\nx <- 34\ny <- 16\nz <- x + y   # addition\nw <- y / x     # division\n# display the results\nx\ny\nz\nw\n# change x\nx <- \"some text\"\n# display the results\nx\ny\nz\nw"},{"path":"start.html","id":"saving-an-r-script","chapter":"Getting started in R and Rstudio","heading":"Saving an R script","text":"can save script clicking Save icon top Script Editor panel. , Save File dialog open.","code":""},{"path":"start.html","id":"executing-code-in-an-r-script","chapter":"Getting started in R and Rstudio","heading":"Executing code in an R script","text":"can run code R script easily. Run button Script Editor panel toolbar run either current line code block selected code. can use First script.R code gain familiarity functionality.Place cursor anywhere line 3 script \\[x = 34\\]. Now press Run button Script Editor panel toolbar. Three things happen: 1) code transferred command console, 2) code executed, 3) cursor moves next line script. Press Run button three times. RStudio executes lines 4, 5, 6 script.Now run set code commands . Highlight lines 8, 9, 10, 11 script.Highlighting accomplished similar may familiar word processor applications. click left mouse button beginning text want highlight, hold mouse button drag cursor end text release button. four lines code highlighted, click editor Run button. four lines code executed command console. takes run script code RStudio.","code":""},{"path":"start.html","id":"comments-in-an-r-script-documenting-your-code","chapter":"Getting started in R and Rstudio","heading":"Comments in an R script (documenting your code)","text":"finishing topic, one final concept understand. always good idea place comments code. help understand code meant . become helpful reopen code wrote weeks ago trying work . saying, “Real programmers document code. hard write, hard understand” meant dark joke, coding style guide.comment R code begins # symbol. code First script.R contains several examples comments. Lines 1, 2, 7, 12, 14 image comment lines. line text starts # treated comment ignored code execution. Lines 5 6 image contain comments end. text # treated comment ignored execution.Notice RStudio editor shows comments colored green. green color helps focus code get confused comments.Besides using comments help make R code easily understood, can use # symbol ignore lines code developing code stream. Simply place # front line want ignore. R treat lines comments ignore . want include lines code execution, remove # symbols code executable . technique allows change code execute without retype deleted code.","code":""},{"path":"data-sets.html","id":"data-sets","chapter":"Data sets","heading":"Data sets","text":"practical sessions make use pre-prepared data. find data required second phase practicals data folder can download .can download files individually website, using bash commands either going terminal tab RStudio opening dedicated terminal. can either copy data Navigate folder want download data , use following commands:","code":""},{"path":"data-sets.html","id":"data-lr","chapter":"Data sets","heading":"Data: Linear regression","text":"","code":"# Stork download\nwget https://github.com/anasrana/ems-practicals/raw/main/data/stork.txt\n\n# lr_data1\nwget https://github.com/anasrana/ems-practicals/raw/main/data/lr_data1.Rdata\n\n# lr_data2\nwget https://github.com/anasrana/ems-practicals/raw/main/data/lr_data2.Rdata"},{"path":"data-sets.html","id":"data-pca","chapter":"Data sets","heading":"Data: PCA","text":"","code":"# Pollen2014 data\nwget https://github.com/anasrana/ems-practicals/raw/main/data/Pollen2014.txt\n\n# Supplementary Labels\nwget https://github.com/anasrana/ems-practicals/raw/main/data/SupplementaryLabels.txt"},{"path":"data-sets.html","id":"data-glm","chapter":"Data sets","heading":"Data: Generalised linear models","text":"","code":"# gwas-cc-ex1.Rdata\nwget https://github.com/anasrana/ems-practicals/raw/main/data/gwas-cc-ex1.Rdata\n\n# gwas-cc-ex2.Rdata\nwget https://github.com/anasrana/ems-practicals/raw/main/data/gwas-cc-ex2.Rdata\n\n# nb_data.Rdata\nwget https://github.com/anasrana/ems-practicals/raw/main/data/nb_data.Rdata"},{"path":"simulating-random-numbers.html","id":"simulating-random-numbers","chapter":"1 Simulating random numbers","heading":"1 Simulating random numbers","text":"number functions R can use simulate random numbers according different probability distributions.function sample allows take sample specified size elements vector xusing sampling without replacement. can use ?sample read documentation describing command.following, use sample function make 10,000 draws set numbers 1, 2, 3 4 display distribution sampled values using histogram.First, define vector called x contains numbers 1, 2, 3, 4. function c allows us combine four numbers together one vector:now use function sample pick four numbers x 10,000 times. result, 10,000 numbers chosen, store :Lets plot histogram values picked:picked number equal probability histogram shows number equally likely chosen.","code":"x <- c(1, 2, 3, 4)out <- sample(x, 10000, replace=TRUE)hist_out <- hist(out, main = '', xlab = 'Values', ylab = 'Frequency')"},{"path":"simulating-random-numbers.html","id":"sim-ex1","chapter":"1 Simulating random numbers","heading":"1.1 Exercise - replacement","text":"","code":""},{"path":"simulating-random-numbers.html","id":"question","chapter":"1 Simulating random numbers","heading":"1.1.1 Question","text":"difference output out1 out2 following piece code?","code":"x <- c( 1, 2, 2, 3, 4, 1, 6, 7, 8, 10, 5, 5, 1, 4, 9 )\nout1 <- sample(x, 10, replace=FALSE)\nout2 <- sample(x, 10, replace=TRUE)"},{"path":"simulating-random-numbers.html","id":"exercise","chapter":"1 Simulating random numbers","heading":"1.2 Exercise","text":"Use sample sample.int function simulate values rolls unbiased six-sided die. Show distribution values obtain consistent unbiased die.Hint 1: Type ?sample.int console get help function.Hint 2: may find useful use function table. Type ?table console get help function.","code":""},{"path":"markov-chains.html","id":"markov-chains","chapter":"2 Markov Chains","heading":"2 Markov Chains","text":"now look Markov Chain. covered lectures based basic principles covered able use simulations.random process known Markov property (Markov process) probability going next state depends current state past states. Markov process memoryless property store property memory past states.Markov process operates within specific (finite) set states, called Markov Chain.Markov Chain defined three properties:state space: set values states process existA state space: set values states process existA transition matrix: defines probability moving one state another stateA transition matrix: defines probability moving one state another stateA current state probability distribution: defines probability one states start processA current state probability distribution: defines probability one states start processConsider following example two states describing weather particular day: () Sunny (ii) Rainy. arrow denotes probability going one state another course day. example, currently sunny, probability raining next day 0.6. Conversely, raining, probability become sunny next day 0.7 0.3 continue raining.transition matrix can written following R:creates 2 x 2 matrix consisting transition probabilities shown diagram.Suppose want simulate sequence 30 days weather patterns days. Assuming day 0 currently sunny, can following:","code":"transitionMatrix = matrix(c(0.4, 0.6, 0.7, 0.3), nrow=2, ncol=2, byrow=TRUE)\nprint(transitionMatrix)#>      [,1] [,2]\n#> [1,]  0.4  0.6\n#> [2,]  0.7  0.3# initial state - it is [1] sunny or [2] rainy\nstate <- 1\nweather_sequence <- rep(0, 30) # vector to store simulated values\nfor (day in 1:30) { # simulate for 30 days\n  pr <- transitionMatrix[state, ] # select the row of transition probabilities\n\n  # sample [1] or [2] based on the probs pr\n  state = sample(c(1, 2), size = 1, prob = pr)\n  weather_sequence[day] <- state # store the sampled state\n}\n\n# print the simulated weather sequence\nprint(weather_sequence)#>  [1] 2 1 1 1 2 2 2 1 2 1 2 1 2 1 2 1 2 2 2 1 1 1 2 1 2 1 1 2\n#> [29] 2 2"},{"path":"markov-chains.html","id":"exercise-mc","chapter":"2 Markov Chains","heading":"2.1 Exercise MC","text":"Can extend example three-state model?Note, diagram (intentionally) misses self-transitions. able infer probabilities given otherwise add one!","code":""},{"path":"a-monopoly-simulation.html","id":"a-monopoly-simulation","chapter":"3 A Monopoly simulation","heading":"3 A Monopoly simulation","text":"Now use simulate simplified games Monopoly (https://en.wikipedia.org/wiki/Monopoly_(game)). addition, also many tutorials guides Web describing produce computer simulations Monopoly. welcome read use examples inspire work.","code":""},{"path":"a-monopoly-simulation.html","id":"moving-around-the-board","chapter":"3 A Monopoly simulation","heading":"3.1 Moving around the board","text":"Monopoly board 40 spaces. Players take turns roll two dice traverse around board according sum dice values.Use following code example simulate turns single player:increasing number turns taken, distribution set simulated board positions converge towards? Show graphically using histogram function.","code":"num_turns <- 100000 # number of turns to take\n\ncurrent_board_position <- 0 # start on the GO space\n\nmove_size <- rep(0, num_turns)\npositions_visited <- rep(0, num_turns)\n\n# use a for loop to simulate a number of turns\nfor (turn in 1:num_turns) {\n\n  # roll two dice\n  die_values <- sample(c(1:6), 2, replace = TRUE)\n\n  # move player position\n\n  # number of positions to move\n  plus_move <- sum(die_values)\n\n  # compute new board position\n  new_board_position <- current_board_position + plus_move\n\n  # update board position (this corrects for the fact the board is circular)\n  current_board_position <- (new_board_position %% 40)\n\n  # store position visited\n  positions_visited[turn] <- current_board_position\n\n}hist(positions_visited, breaks = seq(0, 40, len = 41), right = FALSE)"},{"path":"a-monopoly-simulation.html","id":"going-to-jail","chapter":"3 A Monopoly simulation","heading":"3.2 Going to Jail","text":"player lands Go Jail space must move immediately Jail space. Extend code include possibility going jail. , assume jail, player continues normal next turn.","code":" num_turns <- 100000 # number of turns to take\n\ncurrent_board_position <- 0 # start on the GO space\ngo_to_jail_position <- 30 # the go to jail space\njail_position <- 10 # jail space\n\nmove_size <- rep(0, num_turns)\npositions_visited <- rep(0, num_turns)\n\n# use a for loop to simulate a number of turns\nfor (turn in 1:num_turns) {\n\n  # roll two dice\n  die_values <- sample(c(1:6), 2, replace = TRUE)\n\n  # move player position\n\n  # number of positions to move\n  plus_move <- sum(die_values)\n\n  # compute new board position\n  new_board_position <- current_board_position + plus_move\n\n  # if land on GO TO JAIL square, then go backwards to the JAIL square\n  if (new_board_position == go_to_jail_position) {\n    new_board_position <- jail_position\n  }\n\n  # update board position (this corrects for the fact the board is circular)\n  current_board_position <- (new_board_position %% 40)\n\n  # store position visited\n  positions_visited[turn] <- current_board_position\n\n}"},{"path":"a-monopoly-simulation.html","id":"exercise-1","chapter":"3 A Monopoly simulation","heading":"3.3 Exercise","text":"distribution board positions long game?Can explain result qualitatively?Discuss neighbour instructor.","code":"hist(positions_visited, breaks = seq(0, 40, len = 41), right = FALSE)"},{"path":"a-monopoly-simulation.html","id":"exercise-2","chapter":"3 A Monopoly simulation","heading":"3.4 Exercise","text":"can also go jail, roll three doubles (dice value) row. Update code allow possibility going Jail three doubles. distribution board positions change?","code":""},{"path":"a-monopoly-simulation.html","id":"exercises-extend-the-game","chapter":"3 A Monopoly simulation","heading":"3.5 Exercises: Extend the game","text":"Now consider building complex Monopoly simulation incorporating complex aspects game :purchase propertiesa ledger playerchance community cardsYou need think carefully simplifying assumptions make make task achievable. -ambitious. example, might initially assume players build houses/hotels properties.questions answer simulations:many turns take properties purchased?best properties buy?long take winner determined?\n\nShow Solution\n","code":""},{"path":"monte-carlo-methods.html","id":"monte-carlo-methods","chapter":"4 Monte Carlo Methods","heading":"4 Monte Carlo Methods","text":"Monte Carlo (MC) simulations provide means model problem apply brute force computational power achieve solution - randomly simulate model get answer. idea similar went lecture. best way explain just run bunch examples, lets go!bad! Monte Carlo estimate close true value.","code":""},{"path":"monte-carlo-methods.html","id":"integration","chapter":"4 Monte Carlo Methods","heading":"4.1 Integration","text":"start basic integration. required know perform integration hand, context continuous RVs understand perform integration using MC means. Throughout exercise try relate back saw lecture information integral provides.Suppose instance Normal distribution mean 1 standard deviation 2 want find integral (area curve) 1 3:\\[\n\\int_1^3 \\frac{1}{10 \\sqrt{2\\,\\pi}}\\, e^{- \\frac{(x - 1)^2}{2\\times 2^2}}dx\n\\]can visualise follows:done calculus - worry. going write Monte Carlo approach estimating integral require knowledge calculus!method relies able generate samples distribution counting many values fall 1 3. proportion samples fall range total number samples gives area.First, create new R script Rstudio. Next define number samples obtain. Lets start choosing 1,000Now use R function rnorm simulate 100 numbers Normal distribution mean 1 standard deviation 2:Lets estimate integral 1 3 counting many samples value range:result get :exact answer given using cumulative distribution function pnorm R given :pnorm gives integral Normal distribution (case mean 1 standard deviation 2) negative infinity value specified q.first call pnorm(q=3, mean=1, sd=2) gives us integral:second call pnorm(q=1, mean=1, sd=2) gives us integral:Therefore difference gives us integral interest.","code":"n <- 1000 # number of samples to takesims <- rnorm(n, mean = 1, sd = 2) # simulated normally distributed numbers# find proportion of values between 1-3\nmc_integral <- sum(sims >= 1 & sims <= 3) / nprint(mc_integral)#> [1] 0.341mc_exact = pnorm(q=3, mean=1, sd=2) - pnorm(q=1, mean=1, sd=2)\nprint(mc_exact)#> [1] 0.3413447"},{"path":"monte-carlo-methods.html","id":"mc-accuracy","chapter":"4 Monte Carlo Methods","heading":"4.2 Exercise: MC accuracy","text":"Try increasing number simulations see accuracy improves?Can draw graph number MC samples vs accuracy?","code":""},{"path":"monte-carlo-methods.html","id":"approximating-the-binomial-distribution","chapter":"4 Monte Carlo Methods","heading":"4.3 Approximating the Binomial Distribution","text":"flip coin 10 times want know probability getting 3 heads. trivial problem using Binomial distribution suppose forgotten never learned first place.Lets solve problem Monte Carlo simulation. use common trick representing tails 0 heads 1, simulate 10 coin tosses 100 times see often happens.MC estimate probability \\(P(X>3)\\) getwhich can compare R’s built-Binomial distribution function:","code":"runs <- 100 # number of simulations to run\n\ngreater_than_three <- rep(0, runs) # vector to hold outcomes\n\n# run 100 simulations\nfor (i in 1:runs) {\n\n  # flip a coin ten times (0 - tail, 1 - head)\n  coin_flips <- sample(c(0, 1), 10, replace = T)\n\n  # count how many heads and check if greater than 3\n  greater_than_three[i] <- (sum(coin_flips) > 3)\n}\n\n# compute average over simulations\npr_greater_than_three <- sum(greater_than_three) / runsprint(pr_greater_than_three)#> [1] 0.82print(pbinom(3, 10, 0.5, lower.tail = FALSE))#> [1] 0.828125"},{"path":"monte-carlo-methods.html","id":"problem-mc-binomial","chapter":"4 Monte Carlo Methods","heading":"4.4 Problem: MC Binomial","text":"Try increasing number simulations see accuracy improves?Can plot accuracy varies function number simulations? (hint: see previous section)","code":""},{"path":"monte-carlo-methods.html","id":"monte-carlo-expectations","chapter":"4 Monte Carlo Methods","heading":"4.5 Monte Carlo Expectations","text":"Now consider slightly different problem. Consider following spinner. spinner spun randomly probability 0.5 landing yellow 0.25 landing red blue respectively.rules game landing yellow gain 1 point,* red* lose 1 point blue gain 2 points. can easily calculate expected score. THINKHow relate probabilities? random variable type RV dealing ?Let \\(X\\) denote random variable associated score spin :\\[\n    E[X] = \\frac{1}{2} \\times 1 + \\frac{1}{4} \\times (-1) + \\frac{1}{4} \\times 2 = 0.75\n\\]","code":""},{"path":"monte-carlo-methods.html","id":"mc-expectation-1","chapter":"4 Monte Carlo Methods","heading":"4.6 Exercise: MC Expectation 1","text":"ask challenging question :20 spins probability less 0 points?“might solve ?course, methods analytically solve type problem time even explained already written simulation!solve Monte Carlo simulation need sample Spinner 20 times, return 1 0 wise return 0. repeat 10,000 times see often happens!","code":""},{"path":"monte-carlo-methods.html","id":"using-functions","chapter":"4 Monte Carlo Methods","heading":"4.7 Using Functions","text":"First, going introduce concept function. encountered already module. piece code encapsulated can refer repeated via name function rather repeatedly writing lines code. like learn functions R, can read tutorial software carpentry lesson.function write simulate one game indicated return whether number points less zero.","code":"# simulates a game of 20 spins\nplay_game <- function(){\n    # picks a number from the list (1, -1, 2)\n    # with probability 50%, 25% and 25% twenty times\n  results <- sample(c(1, -1, 2), 20, replace = TRUE, prob = c(0.5, 0.25, 0.25))\n\n  # function returns whether the sum of all the spins is < 1\n  return(sum(results) < 0)\n}"},{"path":"monte-carlo-methods.html","id":"simulating-from-function","chapter":"4 Monte Carlo Methods","heading":"4.8 Simulating from function","text":"Now can use function loop play game 100 times:can compute probability , twenty spins, less zero points:probability low. surprising since 25% chance getting point deduction spin 75% chance gaining points. Try increase number simulation runs see can detect games find negative score.","code":"runs <- 100 # play the game 100 times\n\nless_than_zero <- rep(0, runs) # vector to store outcome of each game\nfor (it in 1:runs) {\n  # play the game by calling the function and store the outcome\n  less_than_zero[it] <- play_game()\n}prob_less_than_zero <- sum(less_than_zero)/runs\nprint(prob_less_than_zero)#> [1] 0"},{"path":"monte-carlo-methods.html","id":"mc-expectation-2","chapter":"4 Monte Carlo Methods","heading":"4.9 Exercise: MC Expectation 2","text":"Modify code allow calculate expected number points 20 spins.Simulate game maximum 20 spins go “bust” hit negative score take account compute expected end game score.","code":""},{"path":"solution-monte-carlo.html","id":"solution-monte-carlo","chapter":"Solution: Monte Carlo","heading":"Solution: Monte Carlo","text":"","code":""},{"path":"solution-monte-carlo.html","id":"solution-mc-accuracy","chapter":"Solution: Monte Carlo","heading":"4.10 Solution: MC accuracy","text":"First let’s increase number simulations accuracyNext, plot results. make use ggplot2 library create nice plots without much effort. input need data.frame need create one based data.shows number Monte Carlo samples increased, accuracy increases (.e. difference estimated integral value real values converges zero). addition, variability integral estimates across different simulation runs reduces.","code":"sample_sizes <- c(10, 50, 100, 250, 500, 1000) # try different sample sizes\nn_sample_sizes <- length(sample_sizes) # number of sample sizes to try\nrpts <- 100 # number of repeats for each sample size\naccuracy <- rep(0, n_sample_sizes) # vector to record accuracy values\naccuracy_sd <- rep(0, n_sample_sizes) # vector to record accuracy sd values\n\n# for each sample size\nfor (i in 1:n_sample_sizes) {\n\n  sample_sz <- sample_sizes[i] # select a sanmple size to use\n\n  # vector to store results from each repeat\n  mc_integral <- rep(0, rpts)\n  for (j in 1:rpts){\n    # simulated normally distributed numbers\n    sims <- rnorm(sample_sz, mean = 1, sd = 2)\n    # find proportion of values between 1-3\n    mc_integral[j] <- sum(sims >= 1 & sims <= 3) / sample_sz\n  }\n\n  # compute average difference between integral estimate and real value\n  accuracy[i] <- mean(mc_integral - mc_exact)\n  # compute sd difference between integral estimate and real value\n  accuracy_sd[i] <- sd(mc_integral - mc_exact)\n\n}\n\nprint(accuracy)#> [1]  0.0226552539 -0.0147447461  0.0006552539  0.0012552539\n#> [5]  0.0015352539  0.0021952539print(accuracy_sd)#> [1] 0.14669421 0.05963221 0.04313474 0.02986502 0.02334341\n#> [6] 0.01566275print(accuracy + accuracy_sd)#> [1] 0.16934947 0.04488746 0.04379000 0.03112027 0.02487866\n#> [6] 0.01785800# load ggplot\nlibrary(ggplot2)\n\n# create a data frame for plotting\ndf <- data.frame(sample_sizes, accuracy, accuracy_sd)\n\nprint(df)#>   sample_sizes      accuracy accuracy_sd\n#> 1           10  0.0226552539  0.14669421\n#> 2           50 -0.0147447461  0.05963221\n#> 3          100  0.0006552539  0.04313474\n#> 4          250  0.0012552539  0.02986502\n#> 5          500  0.0015352539  0.02334341\n#> 6         1000  0.0021952539  0.01566275# use ggplot to plot lines for the mean accuracy and error bars\n# using the std dev\nggplot(df, aes(x = sample_sizes, y = accuracy)) +\n  geom_line() +\n  geom_point() +\n  geom_errorbar(\n      aes(ymin = accuracy - accuracy_sd, ymax = accuracy + accuracy_sd),\n          width = .2,\n          position = position_dodge(0.05)) +\n  ylab(\"Estimate-Exact\") +\n  xlab(\"Run\")"},{"path":"solution-monte-carlo.html","id":"mc-expectation","chapter":"Solution: Monte Carlo","heading":"4.11 MC Expectation","text":"","code":""},{"path":"solution-monte-carlo.html","id":"solution-mc-expectation-1","chapter":"Solution: Monte Carlo","heading":"4.11.1 Solution: MC Expectation 1","text":"","code":"# simulates a game of 20 spins\nplay_game <- function() {\n    # picks a number from the list (1, -1, 2)\n    #  with probability 50%, 25% and 25% twenty times\n  results <- sample(c(1, -1, 2), 20, replace = TRUE, prob = c(0.5, 0.25, 0.25))\n  return(sum(results)) # function returns the sum of all the spins\n}\n\nscore_per_game <- rep(0, runs) # vector to store outcome of each game\nfor (it in 1:runs) {\n  score_per_game[it] <- play_game() # play the game by calling the function\n}\nexpected_score <- mean(score_per_game) # average over all simulations\n\nprint(expected_score)#> [1] 15.11"},{"path":"solution-monte-carlo.html","id":"solution-mc-expectation-2","chapter":"Solution: Monte Carlo","heading":"4.11.2 Solution: MC Expectation 2","text":"games score zero now corresponds number games went bust (genuinely ended game zero).","code":"# simulates a game of up to 20 spins\nplay_game <- function() {\n    # picks a number from the list (1, -1, 2)\n    #  with probability 50%, 25% and 25% twenty times\n  results <- sample(c(1, -1, 2), 20, replace = TRUE, prob = c(0.5, 0.25, 0.25))\n  results_sum <- cumsum(results) # compute a running sum of points\n  # check if the game goes to zero at any point\n  if (sum(results_sum <= 0)) {\n    return(0) # return zero\n  } else {\n    return(results_sum[20]) # returns the final score\n  }\n}\n\ngame_score <- rep(0, runs) # vector to store scores in each game played\n\n# for each game\nfor (it in 1:runs) {\n  game_score[it] <- play_game()\n}\n\nprint(mean(game_score))#> [1] 8.37plot(game_score)"},{"path":"maximum-likelihood.html","id":"maximum-likelihood","chapter":"5 Maximum Likelihood","heading":"5 Maximum Likelihood","text":"lectures, saw use brute-force search parameters find maximum likelihood estimate unknown mean Normal distribution given set data. exercise, now look efficiently real life.","code":""},{"path":"maximum-likelihood.html","id":"the-likelihood-function","chapter":"5 Maximum Likelihood","heading":"5.1 The likelihood function","text":"First, going write function compute log-likelihood function given parameters:Note function returns -sum(logF) numerical optimisation algorithm going use finds minimum function. interested maximum likelihood can turn minimisation problem simply negating likelihood.Now, lets assume data captured following vector:","code":"neglogLikelihood <- function(mu, x) {\n  logF = dnorm(x, mean = mu, sd = 1, log = TRUE)\n  return(-sum(logF))\n}x = c(-0.5, 1.0, 0.2, -0.3, 0.5, 0.89, -0.11, -0.71, 1.0, -1.3, 0.84)\nn = length(x)"},{"path":"maximum-likelihood.html","id":"optimisation","chapter":"5 Maximum Likelihood","heading":"5.2 Optimisation","text":"Now, need define initial search value parameter, arbitrarily pick value:Now use R function optim find maximum likelihood estimate. mentioned , optim finds minimum value function case trying find parameter minimises negative log likelihood., says start search mu_init using function logLikelihood defined . optim algorithm use L-BFGS-B search method. parameter allowed take value lower = -Inf upper = Inf. result stored .optimiser run, can see parameter value found:can compare sample meanIt turns theoretically known maximum likelihood estimate, particular problem, sample mean coincide!can visualise . First define array possible values mu case -0.1 0.3 101 values -:use apply function apply logLikelihood function mu values defined. means need use loop:can plot overlay maximum likelihood result:plot shows optim found mu minimises negative log-likelihood.","code":"mu_init = 1.0out <- optim(mu_init, neglogLikelihood, gr = NULL, x, method = \"L-BFGS-B\",\n         lower = -Inf, upper = Inf)print(out$par)#> [1] 0.1372727print(mean(x))#> [1] 0.1372727mu <- seq(-0.1, 0.3, length = 101)neglogL <- apply( matrix(mu), 1, neglogLikelihood, x)plot(mu, neglogL, pch=\"-\")\npoints(out$par, out$value, col=\"red\", pch=0)"},{"path":"maximum-likelihood.html","id":"two-parameter-estimation","chapter":"5 Maximum Likelihood","heading":"5.3 Two-parameter estimation","text":"Now suppose mean variance Normal distribution unknown need search two parameters maximum likelihood estimation.now need modified negative log-likelihood function:Notice pass one argument theta whose elements parameters mu sigma2 unpack within function.Now can run optim time initial parameters values must initialised two values. Furthermore, variance negative, bound possible lower values sigma2 can take setting lower = c(-Inf, 0.001). second argument means sigma2 lower 0.001:can now visualise results creating two-dimensional contour plot. first need generate grid values mu sigma2:Now apply negative log-likehood function grid generate negative log-likelihood value position grid:now use contour function plot results:Excellent! now found maximum likelihood estimates unknown mean variance Normal distribution data assumed drawn . Let’s compare estimates sample mean variance. First, estimates:Now, sample mean variances:Interesting! maximum likelihood estimates return sample mean biased sample variance estimate (normalise \\(n\\)\n\\(n−1\\)). Indeed, turns theoretically, maximum likelihood estimate give biased estimate population variance.","code":"neglogLikelihood2 <- function(theta,x) {\n  mu <- theta[1] # get value for mu\n  sigma2 <- theta[2] # get value for sigma2\n\n  # compute density for each data element in x\n  logF <- dnorm(x, mean = mu, sd = sqrt(sigma2), log = TRUE)\n\n  return(-sum(logF)) # return negative log-likelihood\n}theta_init = c(1, 1)\n\nout <- optim(theta_init, neglogLikelihood2, gr = NULL, x, method = \"L-BFGS-B\",\n        lower = c(-Inf, 0.001), upper = c(Inf, Inf))# one dimensional grid of values for mu\nmu <- seq(-0.1, 1.0, length = 101)\n# one dimensional grid of values for sigma2\nsigma2 <- seq(0.1, 1.0, length = 101)\n\nmu_xx <- rep(mu, each = 101) # replicate this 101 times\nsigma2_yy <- rep(sigma2, times = 101) # replicate this 101 times\n\n# generate grid of values (each row contains a unique combination\n# of mu and sigma2 values)\nmu_sigma_grid <- cbind(mu_xx, sigma2_yy)neglogL2 <- apply(mu_sigma_grid, 1, neglogLikelihood2, x)# convert vector of negative log-likelihood values into a grid\nneglogL2 <- matrix(neglogL2, 101)\n\n# draw contour plot\ncontour(sigma2, mu, neglogL2, nlevels = 50, xlab = \"sigma2\", ylab = \"mu\")\n# overlay the maximum likelihood estimate as a red circle\npoints(out$par[2], out$par[1], col=\"red\")print(out$par[1]) # mu estimate#> [1] 0.1372727print(out$par[2]) # sigma2 estimate#> [1] 0.5569665print(mean(x)) # sample mean#> [1] 0.1372727print(var(x)) # sample variance (normalised by n-1)#> [1] 0.6126618print(var(x)*(n-1)/n) # sample variance (normalised by n)#> [1] 0.5569653"},{"path":"maximum-likelihood.html","id":"ex-mle","chapter":"5 Maximum Likelihood","heading":"5.4 Exercise: MLE","text":"potentially biased coin tossed 10 times number heads recorded. experiment repeated 5 times number heads recorded 3, 2, 4, 5 2 respectively.Can derive maximum likelihood estimate probability obtaining head?","code":""},{"path":"sol-mle.html","id":"sol-mle","chapter":"Solution: Model Answers: MLE","heading":"Solution: Model Answers: MLE","text":"potentially biased coin tossed 10 times number heads recorded. experiment repeated 5 times number heads recorded 3, 2, 4, 5 2 respectively.need choose correct pdf write likelihood function R. case, use binomial distribution. case, one ways write solution.","code":"neglogLikelihood <- function(p, n, x) {\n  # compute density for each data element in x\n  logF <- dbinom(x, n, prob = c(p, 1 - p), log = TRUE)\n  return(-sum(logF)) # return negative log-likelihood\n}\n\nn <- 10 # number of coin tosses\nx <- c(3, 2, 4, 5, 2) # number of heads observed\n\np_init <- 0.5 # initial value of the probability\n\n# run optim to get maximum likelihood estimates\nout <- optim(p_init, neglogLikelihood, gr = NULL, n, x, method = \"L-BFGS-B\",\n  lower = 0.001, upper = 1-0.001)\n\n# create a grid of probability values\np_vals <- seq(0.001, 1 - 0.001, length = 101)\n\n# use apply to compute the negative log-likelihood for each probability value\nneglogL <- apply(matrix(p_vals), 1, neglogLikelihood, n, x)\n\n# plot negative log-likelihood function and overlay maximum (negative)\n# log-likelihood estimate\nplot(p_vals, neglogL, pch = \"-\")\npoints(out$par, out$value, col = \"red\", pch = 0)"},{"path":"confidence-intervals.html","id":"confidence-intervals","chapter":"6 Confidence Intervals","heading":"6 Confidence Intervals","text":"practical session, learn derive confidence intervals particular problem using Monte Carlo simulations.random experiment, sample data collected can estimate population parameter interest. estimate can either point estimate interval estimate - range values.confidence interval interval estimate associated confidence level. confidence level tells us probability procedure used construct confidence interval result interval containing true population parameter. probability population parameter lies range.counter-intuitive concept shall now illustrate exercise.","code":""},{"path":"confidence-intervals.html","id":"setup","chapter":"6 Confidence Intervals","heading":"6.1 Setup","text":"First, create new R script within Rstudio start code preamble. use package ggplot2 plotting. can always create plot using base package plot function ggplot2 powerful plotting package use throughout course.Use install.packages(\"ggplot2\") console window package installed system. running code BearPortal already installed, use function running code machine.Lets define width interval, set 1 initially change later :","code":"library(ggplot2)interval_width <- 1 # width of confidence interval"},{"path":"confidence-intervals.html","id":"simulating-data","chapter":"6 Confidence Intervals","heading":"6.2 Simulating data","text":"now going generate simulated data experiment. create 30 samples Normal distribution mean 2.5 variance 1. true values population parameters. real experiment, know values using simulated data, obviously control .Let define first:Now, generate normally distributed data using R function rnorm,now 30 samples Normal distribution population mean 2.5 variance 1.","code":"# number of data points to generate\nn <- 30\n# population mean\nmu <- 2.5\n# population standard deviation (square root of population variance)\nsigma <- 1.0# generate n values from the  Normal distribution N(mu, sigma)\nx <- rnorm(n, mean = mu, sd = sigma)"},{"path":"confidence-intervals.html","id":"constructing-the-confidence-interval","chapter":"6 Confidence Intervals","heading":"6.3 Constructing the confidence interval","text":"going pretend know population mean value (2.5) used generate dataset try provide interval estimate simulated sample data.Remember, lectures, sample mean \\(\\bar{x}\\) natural point estimate population mean \\(\\mu\\).suitable interval might centred sample mean extend ,Let’s look interval:","code":"x_bar = mean(x) # compute sample meaninterval <- c(x_bar - interval_width / 2, x_bar + interval_width / 2)print(interval)#> [1] 2.089508 3.089508"},{"path":"confidence-intervals.html","id":"exercise-3","chapter":"6 Confidence Intervals","heading":"6.4 Exercise","text":"confidence interval contain true parameter?","code":""},{"path":"confidence-intervals.html","id":"experiment","chapter":"6 Confidence Intervals","heading":"6.5 Experiment","text":"previous experiment examined one simulated dataset fully understand probabilistic interpretation confidence interval just yet. moment, interval calculated either contain population mean .order understand probabilistic interpretation, need generate many data sets, construct confidence intervals see across generated data sets, often intervals cover true population mean.Monte Carlo simulation, need many repeats simulation. Lets define number repeats used:use 1000 simulations initially make code quick run may want make higher later greater accuracy.Now, let us define series interval widths simultaneously test,creates sequence values 0.1 1.0 steps 0.1 vector interval_width:Now, create vector zeros length. use store number times confidence interval specific widths contain true population meanThe hard work now begins. use loop repeat simulation nreps times. Within loop, simulate new data set, compute sample mean check confidence interval contains true population mean. Since using one confidence width, use second loop cycle different widths.can now calculate, width, estimate probability confidence interval width contain population mean.Let’s use ggplot2 plot relationship,Can see interval width \\(0.6\\) \\((\\bar{x} \\pm 0.3)\\) gives confidence interval close 90% probability containing population mean?Remember lectures saw theory says \\(\\bar{x} \\pm 1.65\\,\\frac{\\sigma}{\\sqrt{n}}\\) gives 90% confidence interval?, compute \\(2 \\times 1.65\\, \\frac{\\sigma}{\\sqrt{n}}\\), get?Monte Carlo estimate matches theory!","code":"nreps <- 1000 # number of Monte Carlo simulation runs# define a series of interval widths\ninterval_width <- seq(0.1, 1.0, 0.1)\n# store the number of interval widths generated\nn_interval_widths <- length(interval_width)print(interval_width)#>  [1] 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0# create a vector to store the number of times the population mean is contained\nmu_contained <- rep(0, n_interval_widths)for (replicate in 1:nreps) {\n\n  x <- sigma * rnorm(n) + mu # simulate a data set\n\n  xbar <- mean(x) # compute the sample mean\n\n  # for each interval width that we are testing ...\n  for (j in 1:n_interval_widths) {\n    # check if the interval contains the true mean\n    if ((mu > xbar - 0.5 * interval_width[j]) &\n        (mu < xbar + 0.5 * interval_width[j])) {\n      # if it is, we increment the count by one for this width\n      mu_contained[j] <- mu_contained[j] + 1\n    }\n  }\n\n}probability_mean_contained <- mu_contained / nreps# create a data frame containing the variables we wish to plot\ndf <- data.frame(interval_width = interval_width,\n                 probability_mean_contained = probability_mean_contained)\n\n# initialise the ggplot\nplt <- ggplot(df, aes(x = interval_width, y = probability_mean_contained))\n# create a line plot\nplt <- plt + geom_line()\n# add a horizontal axis label\nplt <- plt + xlab(\"Interval Width\")\n# create a vertical axis label\nplt <- plt + ylab(\"Probability that mu is contained\")\n\nprint(plt) # plot to screenprint(2 * 1.65 * sigma / sqrt(n))#> [1] 0.6024948"},{"path":"confidence-intervals.html","id":"ex-confidence-interval","chapter":"6 Confidence Intervals","heading":"6.6 Exercise: Confidence Interval","text":"Can devise way compute confidence interval population standard deviation?can make use following point estimate sample variance:\\[\n  s^2 = \\frac{1}{n - 1}\\sum_{= 1}^n (x - \\bar{x})^2\n\\]can calculated using sd function R, remember relationship standard deviation variance.","code":""},{"path":"sol-ex-CI.html","id":"sol-ex-CI","chapter":"Model Answer: Confidence Interval","heading":"Model Answer: Confidence Interval","text":"Can devise way compute confidence interval population standard deviation?can make use following point estimate sample variance:\\[\n  s^2 = \\frac{1}{n - 1}\\sum_{= 1}^n (x - \\bar{x})^2\n\\]can calculated using sd function R, remember relationship standard deviation variance.","code":"# create a vector to store the number of times\n# the population variance is contained\nsigma_contained <- rep(0, n_interval_widths)\n\nfor (replicate in 1:nreps) {\n\n  x <- rnorm(n, mean = mu, sd = sigma) # simulate a data set\n\n  sigmabar <- sd(x) # compute the sample standard deviation\n\n  # for each interval width that we are testing ...\n  for (j in 1:n_interval_widths) {\n    # check if the interval contains the true mean\n    if ((sigma > sigmabar - 0.5 * interval_width[j]) &\n        (sigma < sigmabar + 0.5 * interval_width[j])) {\n\n      # if it is, we increment the count by one for this width\n      sigma_contained[j] <- sigma_contained[j] + 1\n    }\n  }\n}\n\nprobability_var_contained <- sigma_contained / nreps\n\n# create a data frame containing the variables we wish to plot\ndf <- data.frame(interval_width = interval_width,\n        probability_var_contained = probability_var_contained)\n\n# initialise the ggplot\nplt <- ggplot(df, aes(x = interval_width, y = probability_var_contained))\n# create a line plot\nplt <- plt + geom_line()\n# add a horizontal axis label\nplt <- plt + xlab(\"Interval Width\")\n# create a vertical axis label\nplt <- plt + ylab(\"Probability that sigma is contained\")\n\n# plot to screen\nprint(plt)print(df)#>    interval_width probability_var_contained\n#> 1             0.1                     0.310\n#> 2             0.2                     0.580\n#> 3             0.3                     0.766\n#> 4             0.4                     0.872\n#> 5             0.5                     0.954\n#> 6             0.6                     0.984\n#> 7             0.7                     0.997\n#> 8             0.8                     1.000\n#> 9             0.9                     1.000\n#> 10            1.0                     1.000"},{"path":"computational-testing-techniques.html","id":"computational-testing-techniques","chapter":"7 Computational Testing Techniques","heading":"7 Computational Testing Techniques","text":"practical look various hypothesis testing techniques R. main focus implement learned lectures. Please make attempt looking models answers. solutions quite straightforward implement R take long complete.","code":""},{"path":"computational-testing-techniques.html","id":"ht-ex1","chapter":"7 Computational Testing Techniques","heading":"7.1 Exercise 1","text":"process filling milk cartons claimed fill carton average 260g.population fill weight known normal, standard deviation 1.65g.random sample five cartons collected, content weighed, yielding following (g.)Construct suitable hypothesis test, 1% significance level, determine whether cartons -filled.","code":"263.9, 266.2, 266.3, 266.8, 265.0"},{"path":"computational-testing-techniques.html","id":"ht-ex2","chapter":"7 Computational Testing Techniques","heading":"7.2 Exercise 2","text":"mean length certain type component assumed 100mm. Concerns raised mean length 100mm.random sample size 45 obtained, yielding \\(\\bar{x}=103.11\\) \\(s=53.5\\).Perform hypothesis test, 5% level, determine whether concerns justified.","code":""},{"path":"computational-testing-techniques.html","id":"ht-ex3","chapter":"7 Computational Testing Techniques","heading":"7.3 Exercise 3","text":"Can write z_test function perform one-sided two-sided location (z) tests?","code":""},{"path":"computational-testing-techniques.html","id":"ht-ex4","chapter":"7 Computational Testing Techniques","heading":"7.4 Exercise 4","text":"manufacturing process yields product quality control specification \\(\\mu_0=5.4\\).random sample size \\(n=5\\) sample mean \\(5.64\\) sample variance \\(0.05\\).Conduct hypothesis test, \\(5%\\) significance level, assess whether process meeting specification.\n\nSolution\n","code":""},{"path":"computational-testing-techniques.html","id":"ht-ex5","chapter":"7 Computational Testing Techniques","heading":"7.5 Exercise 5","text":"Two methods filling standard gas cylinders claimed different.particular, process \\(\\) claimed yield higher pressure process \\(B\\).random sample 72 cylinders filled using process \\(\\), yielding \\(\\bar{x}_A = 88\\) \\(s_A^2 = 4.5\\).random sample 48 cylinders filled process \\(B\\), yielding \\(\\bar{x}_B = 79\\) \\(s_B^2 = 4.2\\).Conduct hypothesis test, 5% significance level, investigate claim.","code":""},{"path":"computational-testing-techniques.html","id":"ht-ex6","chapter":"7 Computational Testing Techniques","heading":"7.6 Exercise 6","text":"Two catalysts available chemical process. Catalyst B cheaper catalyst .Provided catalyst B produces mean yields, preferred.compare methods experiment conducted yielding following data:evidence say two catalysts produce different mean yields? Test 5% significance level","code":""},{"path":"computational-testing-techniques.html","id":"ht-ex7","chapter":"7 Computational Testing Techniques","heading":"7.7 Exercise 7","text":"animal behaviour experiment group 90 rats proceed ramp one three doors.observed counts door :evidence suggest preference specific door? Test 5% significance level.","code":""},{"path":"computational-testing-techniques.html","id":"ht-ex8","chapter":"7 Computational Testing Techniques","heading":"7.8 Exercise 8","text":"number accidents junction per week, \\(Y\\), observed 50 week period, yielding:Test hypothesis, 1% level, \\(Y \\sim \\operatorname{Poisson} (\\lambda)\\)","code":""},{"path":"practical-linear-regression.html","id":"practical-linear-regression","chapter":"8 Practical: Linear regression","heading":"8 Practical: Linear regression","text":"practical go basics linear modeling R well simulating data. practical contains following elements:simulate linear regression modelinvestigate parameterscharacterize prediction accuracycorrelation real world dataWe make use following packages reshape2, ggplo2, bbmle packages. packages installed can use install.packages() function install . confirmed nto already installed.","code":"library(ggplot2)\nlibrary(reshape2)\nlibrary(bbmle)"},{"path":"practical-linear-regression.html","id":"data","chapter":"8 Practical: Linear regression","heading":"8.1 Data","text":"practical require three datasets, information downloading . want download manually can use following links:stork.txt (download)stork.txt (download)lr_data1.Rdata (download)lr_data1.Rdata (download)lr_data2.Rdata (download).lr_data2.Rdata (download).","code":""},{"path":"practical-linear-regression.html","id":"simulating-data-1","chapter":"8 Practical: Linear regression","heading":"8.2 Simulating data","text":"simulate data based simple linear regression model:\\[\ny_i = \\beta_0 + \\beta_1\\, x_i + \\epsilon_i,\n\\]\\((x_i, y_i)\\) represent \\(\\)-th measurement pair \\(= 1, \\ldots, N\\), \\(\\beta_0\\) \\(\\beta_1\\) regression coefficients representing intercept slope respectively. assume noise term \\(\\epsilon_i \\sim N(0, \\sigma^2)\\) normally distributed zero mean variance \\(\\sigma^2\\).\n{-}\nFirst define values parameters linear regression \\((\\beta_0, \\beta_1, \\sigma^2)\\):next step simulate \\(N = 100\\) covariates \\(x_i\\) randomly sampling standard normal distribution:Next simulate error term:Finally parameters variables simulate response variable \\(y\\):plot data using ggplot2 data need data.frame object:define true data y_true true linear relationship covariate response without noise.Now add true values \\(y\\) scatter plot:","code":"b0 <- 10 # regression coefficient for intercept\nb1 <- -8 # regression coefficient for slope\nsigma2 <- 0.5 # noise varianceset.seed(198) # set a seed to ensure data is reproducible\nN <- 100 # no of data points to simulate\nx <- rnorm(N, mean = 0, sd = 1) # simulate covariate# simulate the noise terms, rnorm requires the standard deviation\ne <- rnorm(N, mean = 0, sd = sqrt(sigma2))# compute (simulate) the response variable\ny = b0 + b1 * x + e# Set up the data point\nsim_data <- data.frame(x = x, y = y)\n\n# create a new scatter plot using ggplot2\nggplot(sim_data, aes(x = x, y = y)) +\n  geom_point()# Compute true y values\ny_true <- b0 + b1 * x\n\n# Add the data to the existing data frame\nsim_data$y_true <- y_truelr_plot <- ggplot(sim_data, aes(x = x, y = y)) +\n  geom_point() +\n  geom_line(aes(x = x, y = y_true), colour = \"red\")\n\nprint(lr_plot)"},{"path":"practical-linear-regression.html","id":"fitting-simple-linear-regression-model","chapter":"8 Practical: Linear regression","heading":"8.3 Fitting simple linear regression model","text":"","code":""},{"path":"practical-linear-regression.html","id":"least-squared-estimation","chapter":"8 Practical: Linear regression","heading":"8.3.1 Least squared estimation","text":"Now simulated data can use regress \\(y\\) \\(x\\), since simulated data know parameters can make comparison. R can use function lm() , default implements least squares estimate:output lm() object (case ls_fit) contains multiple variables. access built functions, e.g. coef(), residuals(), fitted(). explore turn:estimated parameters plot shows good correspondence fitted regression parameters true relationship \\(y\\) \\(x\\). can check plotting residuals, data stored residuals parameter ls_fit object.better way summarising data visualise histogram:expect mean variance residuals close level used generate data.expected since subtracting good fit data leaves \\(\\epsilon\\) \\(0\\) mean \\(0.5\\) variance.","code":"# Use the lm function to fit the data\nls_fit <- lm(y ~ x, data = sim_data)\n\n# Display a summary of fit\nsummary(ls_fit)#> \n#> Call:\n#> lm(formula = y ~ x, data = sim_data)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -1.69905 -0.41534  0.02851  0.41265  1.53651 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  9.95698    0.06701   148.6   <2e-16 ***\n#> x           -7.94702    0.07417  -107.1   <2e-16 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.6701 on 98 degrees of freedom\n#> Multiple R-squared:  0.9915, Adjusted R-squared:  0.9914 \n#> F-statistic: 1.148e+04 on 1 and 98 DF,  p-value: < 2.2e-16# Extract coefficients as a named vector\nls_coef <- coef(ls_fit)\n\nprint(ls_coef)#> (Intercept)           x \n#>    9.956981   -7.947016# Extract intercept and slope\nb0_hat <- ls_coef[1] # alternative ls_fit$coefficients[1]\nb1_hat <- ls_coef[2] # alternative ls_fit$coefficients[2]\n\n# Generate the predicted data based on estimated parameters\ny_hat <- b0_hat + b1_hat * x\nsim_data$y_hat <- y_hat # add to the existing data frame\n\n# Create scatter plot and lines for the original and fitted\nlr_plot <- ggplot(sim_data, aes(x = x, y = y)) +\n  geom_point() +\n  geom_line(aes(x = x, y = y_true), colour = \"red\", size = 1.3) +\n  # plot predicted relationship in blue\n  geom_line(aes(x = x, y = y_hat), colour = \"blue\")#> Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n#> ℹ Please use `linewidth` instead.\n#> This warning is displayed once every 8 hours.\n#> Call `lifecycle::last_lifecycle_warnings()` to see where this warning was\n#> generated.# force Rstudio to display the plot\n  print(lr_plot)# Residuals\nls_residual <- residuals(ls_fit) # can also be accessed via ls_fit$residuals\n\n# scatter plot of residuals\nplot(ls_residual)hist(ls_residual)print(mean(ls_residual))#> [1] 1.776357e-17print(var(ls_residual))#> [1] 0.4444955"},{"path":"practical-linear-regression.html","id":"maximum-likelihood-estimation","chapter":"8 Practical: Linear regression","heading":"8.3.2 Maximum likelihood estimation","text":"Next look maximum likelihood estimation based data simulated earlier. bit involved requires explicitly write function wish minimise. function use part bbmle package.estimated parameters using maximum likelihood also good estimate true values.","code":"# Loading the required package\nlibrary(bbmle)\n\n# function that will be minimised. It takes as arguments all parameters\n# Here we are helped by the way R works we don't have to explicitly pass x.\n# The function will use the existing estimates in the environment\nmle_ll <- function(beta0, beta1, sigma) {\n  # first we predict the response variable based on the guess for our response\n  y_pred = beta0 + beta1 * x\n\n  # next we calculate the normal distribution based on the predicted value\n  # the guess for sigma and return the log\n  log_lh <- dnorm(y, mean = y_pred, sd = sigma, log = TRUE)\n\n  #  We returnr the negative sum of the log likelihood\n  return(-sum(log_lh))\n}\n\n# This is the function that actually performs the estimation\n# The first variable here is the function we will use\n# The second variable passed is a list of initial guesses of parameters\nmle_fit <- mle2(mle_ll, start = list(beta0 = -1, beta1 = 20, sigma = 10))\n\n# With the same summary function as above we can output a summary of the fit\nsummary(mle_fit)#> Maximum likelihood estimation\n#> \n#> Call:\n#> mle2(minuslogl = mle_ll, start = list(beta0 = -1, beta1 = 20, \n#>     sigma = 10))\n#> \n#> Coefficients:\n#>        Estimate Std. Error  z value     Pr(z)    \n#> beta0  9.957019   0.066336  150.099 < 2.2e-16 ***\n#> beta1 -7.947005   0.073426 -108.231 < 2.2e-16 ***\n#> sigma  0.663347   0.046904   14.143 < 2.2e-16 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> -2 log L: 201.7011"},{"path":"practical-linear-regression.html","id":"effect-of-variance","chapter":"8 Practical: Linear regression","heading":"8.4 Effect of variance","text":"Now investigate quality predictions simulating data sets seeing variance affects quality fit indicated mean-squared error (mse).start define parameter simulations, number simulations run variance, variance values try.Next write nested loop. first loop variances second loop number repeats. simulate data, perform fit lm(). can use fitted() function resulting object extract fitted values \\(\\hat{y}\\) use compute mean-squared error true value \\(y\\).created matrix store mse values, plot using ggplot2 convert data.frame. can done using melt() function form reshape2 library. can compare results using boxplots.can see variances mse value mse go increasing variance simulation.changes need make function plot accuracy estimated regression coefficients function variance?","code":"# number of simulations for each noise level\nn_simulations <- 100\n\n# A vector of noise levels to try\nsigma_v <- c(0.1, 0.4, 1.0, 2.0, 4.0, 6.0, 8.0)\nn_sigma <- length(sigma_v)\n\n# Create a matrix to store results\nmse_matrix <- matrix(0, nrow = n_simulations, ncol = n_sigma)\n\n# name row and column\nrownames(mse_matrix) <- c(1:n_simulations)\ncolnames(mse_matrix) <- sigma_v# loop over variance\nfor (i in 1:n_sigma) {\n  sigma2 <- sigma_v[i]\n\n  # for each simulation\n  for (it in 1:n_simulations) {\n\n    # Simulate the data\n    x <- rnorm(N, mean = 0, sd = 1)\n    e <- rnorm(N, mean = 0, sd = sqrt(sigma2))\n    y <- b0 + b1 * x + e\n\n    # set up a data frame and run lm()\n    sim_data <- data.frame(x = x, y = y)\n    lm_fit <- lm(y ~ x, data = sim_data)\n\n    # compute the mean squared error between the fit and the actual y's\n    y_hat <- fitted(lm_fit)\n    mse_matrix[it, i] <- mean((y_hat - y)^2)\n\n  }\n}library(reshape2)\n\n# convert the matrix into a data frame for ggplot2\nmse_df <- melt(mse_matrix)\n# rename the columns\nnames(mse_df) <- c(\"Simulation\", \"variance\", \"MSE\")\n\n# now use a boxplot to look at the relationship between\n# mean-squared prediction error and variance\nmse_plt <- ggplot(mse_df, aes(x = variance, y = MSE)) +\n  geom_boxplot(aes(group = variance))\n\nprint(mse_plt)"},{"path":"practical-linear-regression.html","id":"LR-ex1","chapter":"8 Practical: Linear regression","heading":"8.5 Exercise I: Correlation","text":"Read data stork.txt, compute correlation comment .data represents storks (column 1) Oldenburg Germany \\(1930 - 1939\\) number people (column 2).","code":""},{"path":"practical-linear-regression.html","id":"LR-ex2","chapter":"8 Practical: Linear regression","heading":"8.6 Exercise II: Linear Regression","text":"Fit simple linear model two data sets supplied (lr_data1.Rdata lr_data2.Rdata). files \\((x,y)\\) data saved two vectors, \\(x\\) \\(y\\).Download data Canvas, can read R plot following commands:Fit linear model comment differences two data-sets used .","code":"load(\"lr_data1.Rdata\")\nplot(x, y)load(\"lr_data2.Rdata\")\nplot(x, y)"},{"path":"practical-linear-regression.html","id":"LR-ex3","chapter":"8 Practical: Linear regression","heading":"8.7 Exercise III: Linear Regression","text":"Investigate sample size affect quality fit using mse, use code investigating affect variance inspiration.","code":""},{"path":"practical-principal-component-analysis.html","id":"practical-principal-component-analysis","chapter":"9 Practical: Principal component analysis","heading":"9 Practical: Principal component analysis","text":"practical practice ideas outlined lecture Principal Component Analysis (PCA), include computing principal components, visualisation techniques application real data.","code":""},{"path":"practical-principal-component-analysis.html","id":"data-1","chapter":"9 Practical: Principal component analysis","heading":"9.1 Data","text":"practical require three datasets, information downloading . want download manually can use following links:Pollen2014.txt (download)SupplementaryLabels.txt (download)","code":""},{"path":"practical-principal-component-analysis.html","id":"introduction","chapter":"9 Practical: Principal component analysis","heading":"9.2 Introduction","text":"use PCA order explore complex datasets. performing dimensionality reduction can better visualize data many variables. technique probably popular tool applied across bioscience problems (e.g. gene expression problems).many real-world dataset deal high dimensional data, e.g. number individuals can take number health related measurement (called variables). great, however large number variables also means difficult plot data (“raw” format), turn might difficult understand dataset contains interesting patterns/trends/relationships across individuals. Using PCA visualize data “human friendly” fashion.Recall:PCA performs linear transformation data.means input data can visualized new coordinate system. first coordinate (PC 1) variance found first coordinate; subsequent coordinate orthogonal previous one contains larges variance left.principal component associated certain percentage total variation dataset.variables strongly correlated one another, first principal components enable us visualize relationships present dataset.Eigenvectors describe new directions, whereas accompanying eigenvalues tell us much variance data given direction.eigenvector highest eigenvalue called first principal component. second highest eigenvalue correspond second principle component etc.exist \\(d\\) number eigenvalues eigenvectors; \\(d\\) also equal size data (number dimensions).purpose visualization preselect first \\(q\\) components, \\(q < d\\).","code":""},{"path":"practical-principal-component-analysis.html","id":"example-pca-on-the-mtcars-dataset","chapter":"9 Practical: Principal component analysis","heading":"9.3 Example: PCA on the mtcars dataset","text":"many datasets built R. Wed look mtcars dataset. Type ?mtcars get description data. use head() function look first rows; dim() get dimensions data.case \\(32\\) examples (cars case), \\(11\\) features.\nNow can perform principal component analysis, R implemented prcomp() function. can type ?prcomp see description function help possible arguments. set center scale arguments TRUE, recall lecture important. can try perform PCA without scaling centering compare.can use summary() function summarise results PCA, return standard deviation, proportion variance explained principal component, cumulative proportion.Note, Proportion Variance always add \\(1\\). PC1 explain \\(60.08%\\) variance, PC2 explains \\(24.09%\\), means together PC1 PC2 account \\(84.17%\\) variance.Using str() function can see full structure R object, alternatively using ?prcomp Value section. case cars_pca variable list containing several variables, x data represented using new principal components. can now plot data first two principal components:added color based make car. can observe samples (cars) cluster together. look variables decide certain cars models cluster together.created plot using ggplot2 package, also possible using base plot prefer.","code":"library(ggplot2)\nhead(mtcars)#>                    mpg cyl disp  hp drat    wt  qsec vs am\n#> Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1\n#> Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1\n#> Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1\n#> Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0\n#> Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0\n#> Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0\n#>                   gear carb\n#> Mazda RX4            4    4\n#> Mazda RX4 Wag        4    4\n#> Datsun 710           4    1\n#> Hornet 4 Drive       3    1\n#> Hornet Sportabout    3    2\n#> Valiant              3    1dim(mtcars)#> [1] 32 11cars_pca <- prcomp(mtcars, center = TRUE, scale = TRUE)pca_summary <- summary(cars_pca)\nprint(pca_summary)#> Importance of components:\n#>                           PC1    PC2     PC3     PC4\n#> Standard deviation     2.5707 1.6280 0.79196 0.51923\n#> Proportion of Variance 0.6008 0.2409 0.05702 0.02451\n#> Cumulative Proportion  0.6008 0.8417 0.89873 0.92324\n#>                            PC5     PC6    PC7     PC8\n#> Standard deviation     0.47271 0.46000 0.3678 0.35057\n#> Proportion of Variance 0.02031 0.01924 0.0123 0.01117\n#> Cumulative Proportion  0.94356 0.96279 0.9751 0.98626\n#>                           PC9    PC10   PC11\n#> Standard deviation     0.2776 0.22811 0.1485\n#> Proportion of Variance 0.0070 0.00473 0.0020\n#> Cumulative Proportion  0.9933 0.99800 1.0000pca_df <- data.frame(cars_pca$x, make = stringr::word(rownames(mtcars), 1))\n\nggplot(pca_df, aes(x = PC1, y = PC2, col = make)) +\ngeom_point(size = 3) +\nlabs(x = \"PC1 60.08%\",\n     y = \"PC2 24.09 %\",\n     title = \"Principal components for mtcars\") +\ntheme(legend.position = \"bottom\")plot(pca_df$PC1, pca_df$PC2)"},{"path":"practical-principal-component-analysis.html","id":"creating-plots-for-pca","chapter":"9 Practical: Principal component analysis","heading":"9.4 Creating plots for PCA","text":"Next look another representation data, biplot. combination PCA plot data score plot. saw PCA plot previous section biplot add original axis arrows.can see original axis starting origin. Therefore can make observations original variables (e.g. cyl mpg contribute PC1) data points relates axes.","code":"biplot(cars_pca)"},{"path":"practical-principal-component-analysis.html","id":"exercise-i","chapter":"9 Practical: Principal component analysis","heading":"9.5 Exercise I","text":"Now try perform PCA USArrests data also build R. Typing ?USArrests give information data. Perform analysis subset USArrests[, -3] data.","code":""},{"path":"practical-principal-component-analysis.html","id":"example-single-cell-data","chapter":"9 Practical: Principal component analysis","heading":"9.6 Example: Single cell data","text":"can now try apply learned realistic datasets. can download data either canvas using links Pollen2014.txt SupplementaryLabels.txt. dealing single cell RNA-Seq (scRNA-Seq) data, consist \\(300\\) single cells measured across \\(8686\\) genes.Measurements scRNA-Seq data integer counts, data good properties perform transformation data. commonly used transformation RNA-Seq count data \\(\\log_2\\). also transpose data matrix rows representing cells columns representing genes. data can use perform PCA.now use information read label_df variable rename cells.Next perform PCA data extract proportion variance explained component.Think calculation exactly means. can visualise thisWe see first principal components explain significant variance, PC10, limited contribution. Next plot data using first two Principal components .useful create biplot example?Hint: try plotting one see","code":"pollen_df <-read.table(\"Pollen2014.txt\", sep=',', header = T,row.names=1)\n\nlabel_df <-read.table(\"SupplementaryLabels.txt\", sep=',', header = T)\n\npollen_df[1:10, 1:6]#>          Cell_2338_1 Cell_2338_10 Cell_2338_11 Cell_2338_12\n#> MTND2P28          78          559          811          705\n#> MTATP6P1        2053         1958         4922         4409\n#> NOC2L              1          125          126            0\n#> ISG15           2953         4938          580          523\n#> CPSF3L             2           42           19            0\n#> MXRA8              0            0            0            0\n#> AURKAIP1         302          132           64          492\n#> CCNL2              0          235            0           84\n#> MRPL20           330          477          288          222\n#> SSU72            604          869         2046          158\n#>          Cell_2338_13 Cell_2338_14\n#> MTND2P28          384          447\n#> MTATP6P1         2610         3709\n#> NOC2L             487           66\n#> ISG15            2609            1\n#> CPSF3L             37           12\n#> MXRA8               0            0\n#> AURKAIP1           11          182\n#> CCNL2              13           11\n#> MRPL20             44          282\n#> SSU72             530          272dim(pollen_df)#> [1] 8686  300# scRNA-Seq data transformation\npollen_mat <- log2(as.matrix(pollen_df) + 1)\n# transpose the data\npollen_mat <- t(pollen_mat)# Check which columns we have available\ncolnames(label_df)#> [1] \"Cell_Identifier\"        \"Population\"            \n#> [3] \"Cell_names\"             \"TrueLabel_CellLevel\"   \n#> [5] \"Tissue_name\"            \"TrueLabel_TissuelLevel\"# rename rows\nrownames(pollen_mat) <- label_df$Cell_namessc_pca <- prcomp(pollen_mat)\n\n# variance is the square of the standard deviation\npr_var <- sc_pca$sdev^2\n\n# compute the variance explained by each principal component\nprop_var_exp <- pr_var / sum(pr_var)var_exp <- data.frame(variance = prop_var_exp, pc = 1:length(prop_var_exp))\n\nggplot(var_exp[1:30, ], aes(x = pc, y = variance)) +\n    geom_bar(stat = \"identity\") +\n    labs(x = \"Principal Component\",\n         y  = \"Variance explained\")sc_pca_df <- data.frame(sc_pca$x, cell = rownames(sc_pca$x),\n                        var_exp = prop_var_exp)\n\nggplot(sc_pca_df, aes(x = PC1, y = PC2, col = cell)) +\n    geom_point(size = 2) +\n    theme(legend.position = \"bottom\")"},{"path":"practical-multiple-regression.html","id":"practical-multiple-regression","chapter":"10 Practical: Multiple regression","heading":"10 Practical: Multiple regression","text":"Previously considered simple linear regression one response variable one feature. practical go examples multiple features:\\[y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon \\]practical use data already inbuilt R part MASS package. thing need make data available load MASS package.","code":""},{"path":"practical-multiple-regression.html","id":"multiple-regression","chapter":"10 Practical: Multiple regression","heading":"10.1 Multiple regression","text":"part use inbuilt trees dataset containing Volume, Girth Height data 31 trees.First revisit linear regression example, recall function fit linear model lm(). Consider Volume response variable Girth covariate.now consider linear regression example multiple covariates, Girth well Height. case course know related expect covariates significant.Note, formula enter covariates regression coefficients information regarding noise.Let us now look RSS values, can calculate RSS lf_fit object using sum(residuals(lr_fit)^2). see RSS LR = 524.3 RSS MR = 421.92. Therefore fit improved regression coefficient Height small significant.One reason relationship Volume, Girth, Height additive rather Girth Height multiplied. Using fact \\(\\log(*b) = \\log() + \\log(b)\\) can consider log-transformed data linear model.Now see regression coefficient large covariates significant. shows need ensure understand relationship covariates construct model.","code":"lr_fit <- lm(Volume ~ Girth, data = trees)\nsummary(lr_fit)#> \n#> Call:\n#> lm(formula = Volume ~ Girth, data = trees)\n#> \n#> Residuals:\n#>    Min     1Q Median     3Q    Max \n#> -8.065 -3.107  0.152  3.495  9.587 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) -36.9435     3.3651  -10.98 7.62e-12 ***\n#> Girth         5.0659     0.2474   20.48  < 2e-16 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 4.252 on 29 degrees of freedom\n#> Multiple R-squared:  0.9353, Adjusted R-squared:  0.9331 \n#> F-statistic: 419.4 on 1 and 29 DF,  p-value: < 2.2e-16mr_fit <- lm(Volume ~ Girth + Height, data = trees)\n\n\nsummary(mr_fit)#> \n#> Call:\n#> lm(formula = Volume ~ Girth + Height, data = trees)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -6.4065 -2.6493 -0.2876  2.2003  8.4847 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) -57.9877     8.6382  -6.713 2.75e-07 ***\n#> Girth         4.7082     0.2643  17.816  < 2e-16 ***\n#> Height        0.3393     0.1302   2.607   0.0145 *  \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 3.882 on 28 degrees of freedom\n#> Multiple R-squared:  0.948,  Adjusted R-squared:  0.9442 \n#> F-statistic:   255 on 2 and 28 DF,  p-value: < 2.2e-16mrl_fit <- lm(log(Volume) ~ log(Girth) + log(Height), data = trees)\n\nsummary(mrl_fit)#> \n#> Call:\n#> lm(formula = log(Volume) ~ log(Girth) + log(Height), data = trees)\n#> \n#> Residuals:\n#>       Min        1Q    Median        3Q       Max \n#> -0.168561 -0.048488  0.002431  0.063637  0.129223 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) -6.63162    0.79979  -8.292 5.06e-09 ***\n#> log(Girth)   1.98265    0.07501  26.432  < 2e-16 ***\n#> log(Height)  1.11712    0.20444   5.464 7.81e-06 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.08139 on 28 degrees of freedom\n#> Multiple R-squared:  0.9777, Adjusted R-squared:  0.9761 \n#> F-statistic: 613.2 on 2 and 28 DF,  p-value: < 2.2e-16"},{"path":"practical-multiple-regression.html","id":"categorical-covariates","chapter":"10 Practical: Multiple regression","heading":"10.2 Categorical covariates","text":"Recall lecture covariates don’t need numerical can also categorical. now explore regression categorical variable. Load new dataset included MASS package, won’t able load dataset package isn’t installed. Load dataset explore data looks like.give data interpretable names generally cleanup data little bit.Now perform linear regression using categorical variable, different performing linear regression numeric data. difference interpretation.put categorical variable formula lm case bwt_grams ~ mother_smokes two levels categorical variable. consider model \\(y = \\beta_0 + \\beta_1 x + \\epsilon\\)\ncoefficients model can interpreted follows:\\(\\beta_0\\) average birth weight mother non smoker\\(\\beta_0 + \\beta_1\\) average birth weight mother smoker\\(\\beta_1\\) average difference birth weight babies mother smokers mothers non smokers.Categorical variables can also two levels cases additional level can interpreted way.used linear regression general don’t want use simple linear regression categorical variables, can convince case plotting data. use simple example highlight different interpretation coefficients.","code":"library(MASS)\ndata(\"birthwt\")\n\nhead(birthwt)#>    low age lwt race smoke ptl ht ui ftv  bwt\n#> 85   0  19 182    2     0   0  0  1   0 2523\n#> 86   0  33 155    3     0   0  0  0   3 2551\n#> 87   0  20 105    1     1   0  0  0   1 2557\n#> 88   0  21 108    1     1   0  0  1   2 2594\n#> 89   0  18 107    1     1   0  0  1   0 2600\n#> 91   0  21 124    3     0   0  0  0   0 2622summary(birthwt)#>       low              age             lwt       \n#>  Min.   :0.0000   Min.   :14.00   Min.   : 80.0  \n#>  1st Qu.:0.0000   1st Qu.:19.00   1st Qu.:110.0  \n#>  Median :0.0000   Median :23.00   Median :121.0  \n#>  Mean   :0.3122   Mean   :23.24   Mean   :129.8  \n#>  3rd Qu.:1.0000   3rd Qu.:26.00   3rd Qu.:140.0  \n#>  Max.   :1.0000   Max.   :45.00   Max.   :250.0  \n#>       race           smoke             ptl        \n#>  Min.   :1.000   Min.   :0.0000   Min.   :0.0000  \n#>  1st Qu.:1.000   1st Qu.:0.0000   1st Qu.:0.0000  \n#>  Median :1.000   Median :0.0000   Median :0.0000  \n#>  Mean   :1.847   Mean   :0.3915   Mean   :0.1958  \n#>  3rd Qu.:3.000   3rd Qu.:1.0000   3rd Qu.:0.0000  \n#>  Max.   :3.000   Max.   :1.0000   Max.   :3.0000  \n#>        ht                ui              ftv        \n#>  Min.   :0.00000   Min.   :0.0000   Min.   :0.0000  \n#>  1st Qu.:0.00000   1st Qu.:0.0000   1st Qu.:0.0000  \n#>  Median :0.00000   Median :0.0000   Median :0.0000  \n#>  Mean   :0.06349   Mean   :0.1481   Mean   :0.7937  \n#>  3rd Qu.:0.00000   3rd Qu.:0.0000   3rd Qu.:1.0000  \n#>  Max.   :1.00000   Max.   :1.0000   Max.   :6.0000  \n#>       bwt      \n#>  Min.   : 709  \n#>  1st Qu.:2414  \n#>  Median :2977  \n#>  Mean   :2945  \n#>  3rd Qu.:3487  \n#>  Max.   :4990# rename columns\ncolnames(birthwt) <- c(\"bwt_below_2500\", \"mother_age\", \"mother_weight\", \"race\",\n                       \"mother_smokes\", \"previous_prem_labor\", \"hypertension\",\n                       \"uterine_irr\", \"physician_visits\", \"bwt_grams\")\n\nbirthwt$race <- factor(c(\"white\", \"black\", \"other\")[birthwt$race])\nbirthwt$mother_smokes <- factor(c(\"No\", \"Yes\")[birthwt$mother_smokes + 1])\nbirthwt$uterine_irr <- factor(c(\"No\", \"Yes\")[birthwt$uterine_irr + 1])\nbirthwt$hypertension <- factor(c(\"No\", \"Yes\")[birthwt$hypertension + 1])\n\nggplot(birthwt, aes(x = mother_smokes, y = bwt_grams)) +\n    geom_boxplot() +\n    labs(title = \"Data on baby births in Springfield (1986)\",\n         x = \"Does the mother smoke?\",\n         y = \"Birth-weight [grams]\")ggplot(birthwt, aes(x = mother_age, y = bwt_grams, col = mother_smokes)) +\n    geom_point()bwt_fit <- lm(bwt_grams ~ mother_smokes, data = birthwt)\n\nsummary(bwt_fit)#> \n#> Call:\n#> lm(formula = bwt_grams ~ mother_smokes, data = birthwt)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -2062.9  -475.9    34.3   545.1  1934.3 \n#> \n#> Coefficients:\n#>                  Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)       3055.70      66.93  45.653  < 2e-16 ***\n#> mother_smokesYes  -283.78     106.97  -2.653  0.00867 ** \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 717.8 on 187 degrees of freedom\n#> Multiple R-squared:  0.03627,    Adjusted R-squared:  0.03112 \n#> F-statistic: 7.038 on 1 and 187 DF,  p-value: 0.008667"},{"path":"practical-multiple-regression.html","id":"residuals","chapter":"10 Practical: Multiple regression","heading":"10.3 Residuals","text":"Recall lectures residuals differences observed data \\(y\\) fitted values \\(\\hat{y}\\). One assumptions make simple linear regression model residuals normally distributed. extract residuals lm object use residuals() function.Even consider residuals look like normally distributed need get better understanding use Q-Q Plot. can take look wiki get better understanding (Q–Q plot - Wikipedia). simple terms residuals normally distributed expect diagonal straight line Q-Q plot. simplest way get plot using plot() function specifically lm object option = takes numeric value depending plot want plot.can see example residuals close normal outliers especially towards larger values residual. indicate model stands fulfil assumption fully comes close.","code":"residuals_df <-\ndata.frame(resid = residuals(bwt_fit))\n\nggplot(residuals_df, aes(x = resid)) +\n    geom_histogram(bins = 10)plot(bwt_fit, which = 2)"},{"path":"practical-multiple-regression.html","id":"gradient-descent-algorithm","chapter":"10 Practical: Multiple regression","heading":"10.4 Gradient descent algorithm (+)","text":"Finally, todays practical implement gradient descent algorithm discussed lecture.simplicity consider case one covariate. section use simulated data compare results lm(). model simulate :\\[y = 2 + 3 x + \\epsilon\\]Recall gradient descent want minimise Mean Squared Error (\\(J(\\beta)\\)) cost function. first step write cost function R. simplicity use matrix multiplication, R implemented %*%. (Note, get help function special characters can’t simply run command ?%*% instead put quotes ?\"%*%\".)perform optimisation initialise parameters, general optimisation algorithms won’t always produce results choices initialisations.now write loop compute optimisation, store full history opmtimisation.created list store results res possible combine results simple data.frame using bind_rows() function dplyr package. look final values resulting variable willWe can see \\(\\beta_0 = 2\\) \\(\\beta_1 = 3\\) reproduced faithfully. ways visualise optimisation. can look convergence parameters, cost function even estimated \\(y\\) step optimisation.Now compare results ones obtained fitting linear model R using function lm(), different results. Try reproduce plots \\(\\alpha =\\) (0.02, 0.1, 0.5), different number iterations optimisation compare estimated \\(\\hat{\\beta}_0\\), \\(\\hat{\\beta}_1\\) values use simulation step. give idea important right choice two parameters .","code":"# setting seed to be able to reproduce the simulation\nset.seed(200)\n\n# number of samples\nn_sample <- 1000\n\n# We sample x values from a uniform distribution in the range [-5, 5]\nx <- runif(n_sample, -5, 5)\n# Next we compute y\ny <- 3 * x + 2 + rnorm(n = n_sample, mean = 0, sd = 1)\n\nsim_df <- data.frame(x = x, y = y)\n\nggplot(sim_df, aes(x = x, y = y)) +\n    geom_point()cost_fn <- function(X, y, coef) {\n    sum( (X %*% coef - y)^2 ) / (2*length(y))\n}# First we set alpha and the number of iterations we will perform\nalpha <- 0.2\nnum_iters <- 100\n\n# next we will initialise regression coefficients\ncoef <- matrix(c(0,0), nrow=2)\nX <- cbind(1, matrix(x))\nres <- vector(\"list\", num_iters)for (i in 1:num_iters) {\n  error <- (X %*% coef - y)\n  delta <- t(X) %*% error / length(y)\n  coef <- coef - alpha * delta\n  res_df <- data.frame(itr = i , cost = cost_fn(X, y, coef),\n                   b0 = coef[1], b1 = coef[2])\n\n  res[[i]] <- res_df\n}library(dplyr)\nres_df <- bind_rows(res)\ntail(res_df)#>     itr      cost       b0       b1\n#> 95   95 0.5275707 2.034285 3.014512\n#> 96   96 0.5275707 2.034285 3.014512\n#> 97   97 0.5275707 2.034285 3.014512\n#> 98   98 0.5275707 2.034285 3.014512\n#> 99   99 0.5275707 2.034285 3.014512\n#> 100 100 0.5275707 2.034285 3.014512ggplot(res_df, aes(x = itr, y = b1)) +\n    geom_line() +\n    labs(x = \"Iteration\",\n         y = \"Estimated beta_1\",\n         title = \"Visuaslisation of the cconvergence of the beta_1 parameter\")ggplot(res_df, aes(x = itr, y = cost)) +\n    geom_line() +\n    labs(x = \"Iteration\",\n         y = \"Cost function\",\n         title = \"History of cost function at each iteration\")ggplot(sim_df, aes(x = x, y = y)) +\n    geom_point(color = \"red\", alpha = 0.3) +\n    geom_abline(data = res_df, aes(intercept = b0, slope = b1),\n                alpha = 0.3, col = \"darkgreen\", size = 0.5) +\n    labs(x = \"x\", y = \"y\",\n         title = \"Estimated response at each step during optimisation\")#> Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n#> ℹ Please use `linewidth` instead.\n#> This warning is displayed once every 8 hours.\n#> Call `lifecycle::last_lifecycle_warnings()` to see where this warning was\n#> generated."},{"path":"practical-generalised-linear-models.html","id":"practical-generalised-linear-models","chapter":"11 Practical: Generalised linear models","heading":"11 Practical: Generalised linear models","text":"genome-wide association study, perform experiment select \\(n\\) individuals disease (cases) \\(n\\) individuals without diseases (controls) look genetic differences two groups. particular, interested specific genetic variants (SNPs) might induce predisposition towards disease.Suppose observe following genotypes SNP 40,000 individuals (20,000 cases, 20,000 controls):Genotypes: AA Aa aaControls: 3 209 1788Cases: 83 621 1296The cases seem relatively alleles controls. might make us suspect alleles SNP associated disease.","code":""},{"path":"practical-generalised-linear-models.html","id":"data-2","chapter":"11 Practical: Generalised linear models","heading":"11.1 Data","text":"practical use preprepared data-files. download instructions can use links download :gwas-cc-ex1.Rdata (download)gwas-cc-ex2.Rdata (download)nb_data.Rdata (download)","code":""},{"path":"practical-generalised-linear-models.html","id":"detecting-snp-associations","chapter":"11 Practical: Generalised linear models","heading":"11.2 Detecting SNP associations","text":"seen lectures can statistical tests type contingency table using Chi Squared Tests. Let’s load example data set prepareNow need write loop scans , \\(p\\), SNPs:went SNP (rows matrix X), extracted counts genotype (marked 1. code) cases controls, compute expected probability (marked 2. code). Finally, perform chi-squared contingency table test comparing observed counts expected probabilities assuming genotype related disease status (marked 3. code).plot knows Manhattan plot. One SNP (i_p = 250) pop highly associated disease process. Look genotype counts (MAF) SNP cases controls see large difference distribution genotypes (MAF).","code":"library(ggplot2) # for plots later\nload(\"gwas-cc-ex1.Rdata\")\n\n# how many individuals are there\nn <- length(y)\n# How many SNPs do we have data for\np <- nrow(X)\n\n# samples that are controls are encoded as 0 in y\ncontrol <- which(y == 0)\n# disease cases are encoded as 1 in y\ncases <- which(y == 1)# create a vector where p-values will be stored\np_vals <- rep(0, p)\n\n# Loop over SNPs\nfor (i_p in 1:p) {\n    # 1. obtain genotype counts\n    counts <- matrix(0, nrow = 2, ncol = 3)\n    counts[1, ] <- c(sum(X[i_p, control] == 0),\n                     sum(X[i_p, control] == 1),\n                     sum(X[i_p, control] == 2))\n\n    counts[2, ] <- c(sum(X[i_p, cases] == 0),\n                     sum(X[i_p, cases] == 1),\n                     sum(X[i_p, cases] == 2))\n\n    # 2. expected probability of AA\n    # (assuming no dependence on case/control status)\n    expected_pr_AA <- sum(counts[, 1]) / n\n    # expected probability of Aa\n    expected_pr_Aa <- sum(counts[, 2]) / n\n    # expected probability of aa\n    expected_pr_aa <- sum(counts[, 3]) / n\n\n    expected_probs <- c(expected_pr_AA, expected_pr_Aa, expected_pr_aa )\n\n    # 3. do my chi-squared test\n    out <- chisq.test(counts, p = expected_probs)\n    # extract p value of test and store\n    p_vals[i_p] <- out$p.value\n}p_val_df <- data.frame(p_val = p_vals, idx = 1:p)\n\nggplot(p_val_df, aes(x = idx, y = -log10(p_val))) +\n    geom_point(size = 2.5, col = \"dodgerblue1\")i_p <- 250\ncounts_v <- c(sum(X[i_p, control] == 0), sum(X[i_p, control] == 1),\n              sum(X[i_p, control] == 2), sum(X[i_p, cases] == 0),\n              sum(X[i_p, cases] == 1), sum(X[i_p, cases] == 2))\n\nsnp_procs <- data.frame(counts_v, type = rep(c(\"control\", \"cases\"), each = 3),\n           genotype = rep(c(\"AA\", \"Aa\", \"aa\"), 2))\n\nggplot(snp_procs, aes(x = genotype, y = counts_v, fill = type)) +\n    geom_bar(stat = \"identity\", position = \"dodge\")"},{"path":"practical-generalised-linear-models.html","id":"gwas-and-logistic-regression","chapter":"11 Practical: Generalised linear models","heading":"11.3 GWAS and logistic regression","text":"Now lets approach problem using Generalised Linear Models. Lets load data set containing genotypes X case-control status y:p SNPs going call R GLM function glm using binomial family option logit link function outcomes binary. extract p-value associated regression coefficient genotype. obtained applying hypothesis test (Wald Test) whether coefficient null value zero.testing 1,000 SNPs lets use Bonferroni correction adjust p-values take account multiple testing:Lets use adjusted -log10 p-values plot Manhattan plot:see single SNP showing strong association disease status.","code":"# load an example data set (genotypes in X, case-control (1/0) status in y)\nload(\"gwas-cc-ex2.Rdata\")\n\nn <- length(y) # how many individuals do we have in total?\np <- nrow(X) # how many SNPs do I have data for?p_vals <- rep(0, p)\nfor ( j in 1:p ) {\n  snp_data <- data.frame(y = y, x = X[j, ])\n  glm.fit <- glm(y ~ x, family = binomial(link = logit), data = snp_data  )\n  p_vals[j] <- summary(glm.fit)$coefficients[2,4]\n}adj_p_vals <- p.adjust(p_vals, \"bonferroni\")# create data.frame with p-values for plotting with ggplot\np_val_df <- data.frame(p_val = adj_p_vals, idx = 1:p)\n\nggplot(p_val_df, aes(x = idx, y = -log10(p_val))) +\n    geom_point(size = 2.5, col = \"dodgerblue1\") +\n    labs(y = \"-log10(adjusted p-value)\")"},{"path":"practical-generalised-linear-models.html","id":"negative-binomial-and-poisson-regression","chapter":"11 Practical: Generalised linear models","heading":"11.4 Negative binomial and Poisson regression","text":"Molecular biologists study behaviour protein expression normal cancerous tissues. hypothesis total number -expressed proteins depends histopathological-derived tumour subtype immune cell contexture measure.provided data 314 tumours file nb_data.Rdata. file contains one data frame following variables:overexpressed_proteins: response variable interest.immunoscore: gives standardised measure immune cell contexture.tumor_subtype: three-level nominal variable indicating histopathological sub-type tumour. three levels Unstable, Stable, ComplexLet’s load prerequisite R libraries data produce summary statistics (install required using install.package() command ):","code":"# required libraries\nlibrary(MASS)\nlibrary(foreign)#> Warning: package 'foreign' was built under R version 4.3.3load(\"nb_data.Rdata\")\n\n# print summary statistics to Console\nsummary(dat)#>    sample_id      gender     immunoscore   \n#>  1001   :  1   female:160   Min.   : 1.00  \n#>  1002   :  1   male  :154   1st Qu.:28.00  \n#>  1003   :  1                Median :48.00  \n#>  1004   :  1                Mean   :48.27  \n#>  1005   :  1                3rd Qu.:70.00  \n#>  1006   :  1                Max.   :99.00  \n#>  (Other):308                               \n#>  overexpressed_proteins  tumor_subtype\n#>  Min.   : 0.000         Complex : 40  \n#>  1st Qu.: 1.000         Unstable:167  \n#>  Median : 4.000         Stable  :107  \n#>  Mean   : 5.955                       \n#>  3rd Qu.: 8.000                       \n#>  Max.   :35.000                       \n#> "},{"path":"practical-generalised-linear-models.html","id":"count-based-glms","chapter":"11 Practical: Generalised linear models","heading":"11.4.1 Count-based GLMs","text":"overexpressed_proteins measurements counts. implies use Poisson based GLM.Poisson regression models, conditional variance definition equal conditional mean. can limiting.Negative binomial regression can used -dispersed count data, conditional variance exceeds conditional mean.can considered generalization Poisson regression since mean structure Poisson regression extra parameter model -dispersion. conditional distribution outcome variable -dispersed, confidence intervals Poisson regression likely narrower compared Negative Binomial regression model.following try models see fits best.","code":""},{"path":"practical-generalised-linear-models.html","id":"fitting-a-glm","chapter":"11 Practical: Generalised linear models","heading":"11.4.2 Fitting a GLM","text":"use glm.nb function MASS package estimate negative binomial regression. use function similar lm linear models additional requirement link function. count data always positive, log link function useful .R first displays call deviance residuals. Next, see regression coefficients variables, along standard errors, z-scores, p-values. variable immunoscore coefficient -0.006, statistically significant 5% level (Pr(>|z|) = 0.0124*). means one-unit increase immunoscore, expected log count number overexpressed_proteins decreases 0.006.indicator variable shown tumor_subtypeUnstable expected difference log count group Unstable reference group (tumor_subtype=Complex). expected log count Unstable type approximately 0.4 lower expected log count Complex type.indicator variable Stable type expected difference log count Stable type reference Complex group. expected log count Stable approximately 1.2 lower expected log count Complex type.","code":"glm_1 <- glm.nb(overexpressed_proteins ~ immunoscore + tumor_subtype + gender, data = dat, link=log)\n\n# print summary statistics of glm.nb output object to Console\nsummary(glm_1)#> \n#> Call:\n#> glm.nb(formula = overexpressed_proteins ~ immunoscore + tumor_subtype + \n#>     gender, data = dat, link = log, init.theta = 1.047288915)\n#> \n#> Coefficients:\n#>                        Estimate Std. Error z value Pr(>|z|)\n#> (Intercept)            2.707484   0.204275  13.254  < 2e-16\n#> immunoscore           -0.006236   0.002492  -2.502   0.0124\n#> tumor_subtypeUnstable -0.424540   0.181725  -2.336   0.0195\n#> tumor_subtypeStable   -1.252615   0.199699  -6.273 3.55e-10\n#> gendermale            -0.211086   0.121989  -1.730   0.0836\n#>                          \n#> (Intercept)           ***\n#> immunoscore           *  \n#> tumor_subtypeUnstable *  \n#> tumor_subtypeStable   ***\n#> gendermale            .  \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for Negative Binomial(1.0473) family taken to be 1)\n#> \n#>     Null deviance: 431.67  on 313  degrees of freedom\n#> Residual deviance: 358.87  on 309  degrees of freedom\n#> AIC: 1740.3\n#> \n#> Number of Fisher Scoring iterations: 1\n#> \n#> \n#>               Theta:  1.047 \n#>           Std. Err.:  0.108 \n#> \n#>  2 x log-likelihood:  -1728.307"},{"path":"practical-generalised-linear-models.html","id":"comparing-nested-models","chapter":"11 Practical: Generalised linear models","heading":"11.4.3 Comparing nested models","text":"determine tumor_subtype , overall, statistically significant, can compare model without tumor_subtype. reason important fit separate models , unless , overdispersion parameter held constant fair comparison.use anova function compare models using likelihood ratio test (LRT):two degree--freedom chi-square test indicates tumor_subtype statistically significant predictor overexpressed_proteins (Pr(Chi) = 3.133546e-10).anova function performs form LRT. computes likelihood data two models compared uses ration likelihood values test statistic.Theory tells us , large samples sizes, (2x) log likelihood ratio chi-squared distribution degrees freedom equal difference number free parameters two models compared. LRT applies nested models, .e. pair models one less complex subset .","code":"glm_2 <- glm.nb(overexpressed_proteins ~ immunoscore + gender, data = dat, link = log)anova(glm_1, glm_2, test = \"LRT\")#> Warning in anova.negbin(glm_1, glm_2, test = \"LRT\"): only\n#> Chi-squared LR tests are implemented#> Likelihood ratio tests of Negative Binomial Models\n#> \n#> Response: overexpressed_proteins\n#>                                  Model     theta Resid. df\n#> 1                 immunoscore + gender 0.8705939       311\n#> 2 immunoscore + tumor_subtype + gender 1.0472889       309\n#>      2 x log-lik.   Test    df LR stat.      Pr(Chi)\n#> 1       -1772.074                                   \n#> 2       -1728.307 1 vs 2     2 43.76737 3.133546e-10"},{"path":"practical-generalised-linear-models.html","id":"negative-binomial-vs-poisson-glms","chapter":"11 Practical: Generalised linear models","heading":"11.5 Negative-Binomial vs Poisson GLMs","text":"Negative binomial models assume conditional means equal conditional variances. inequality captured estimating dispersion parameter (shown output) held constant Poisson model. Thus, Poisson model actually nested negative binomial model. can use likelihood ratio test compare two models., first fit GLM Poisson regression:Now, lets likelihood ratio test, can extract log-likelihood using logLik() use pchisq() extract probability getting statistic least extreme :Note complex model goes first complex models always larger likelihood.example associated chi-squared value estimated 2*(logLik(m1) – logLik(m3)) around 900 one degree freedom. strongly suggests negative binomial model, estimating dispersion parameter, appropriate Poisson model.","code":"glm_3 <- glm(overexpressed_proteins ~ immunoscore + tumor_subtype + gender, family = \"poisson\", data = dat)pchisq(2 * (logLik(glm_1) - logLik(glm_3)), df = 1, lower.tail = FALSE)#> 'log Lik.' 3.847622e-198 (df=6)"},{"path":"practical-generalised-linear-models.html","id":"further-understanding-the-model-optional","chapter":"11 Practical: Generalised linear models","heading":"11.6 Further understanding the model (OPTIONAL)","text":"assistance understanding model, can look predicted counts various levels predictors. create new datasets values immunoscore tumor_subtype use predict command calculate predicted number overexpressed proteinsFirst, can look predicted counts value tumor_subtype holding immunoscore mean. , create new dataset combinations tumor_subtype immunoscore like find predicted values, use predict() command.","code":"newdata_1 <-\ndata.frame(\n    immunoscore = mean(dat$immunoscore),\n    tumor_subtype = factor(c(\"Complex\", \"Unstable\", \"Stable\"), labels = levels(dat$tumor_subtype)),\n    gender=\"male\")\n\nnewdata_2 <-\ndata.frame(\n    immunoscore = mean(dat$immunoscore),\n    tumor_subtype = factor(c(\"Complex\", \"Unstable\", \"Stable\"), labels = levels(dat$tumor_subtype)),\n    gender=\"female\")\n\nnew_data <- rbind(newdata_1, newdata_2)\n\nnew_data$phat <- predict(glm_1, new_data, type = \"response\")\n\nprint(new_data)#>   immunoscore tumor_subtype gender      phat\n#> 1    48.26752       Complex   male  8.983829\n#> 2    48.26752        Stable   male  2.567187\n#> 3    48.26752      Unstable   male  5.876060\n#> 4    48.26752       Complex female 11.095193\n#> 5    48.26752        Stable female  3.170523\n#> 6    48.26752      Unstable female  7.257042newdata_3 <-\ndata.frame(\n    immunoscore = rep(seq(from = min(dat$immunoscore), to = max(dat$immunoscore), length.out = 100), 3),\n    tumor_subtype = rep(factor(c(\"Complex\", \"Unstable\", \"Stable\"), labels = levels(dat$tumor_subtype)), each=100),\n    gender=\"male\")\n\nnewdata_4 <-\ndata.frame(\n    immunoscore = rep(seq(from = min(dat$immunoscore), to = max(dat$immunoscore), length.out = 100), 3),\n    tumor_subtype = rep(factor(c(\"Complex\", \"Unstable\", \"Stable\"), labels = levels(dat$tumor_subtype)), each=100),\n    gender=\"female\")\n\nnew_data <- rbind(newdata_3, newdata_4)\n\nnew_data <- cbind(new_data, predict(glm_1, new_data, type = \"link\", se.fit=TRUE))\n\nnew_data <- within(new_data, {\n  overexpressed_proteins <- exp(fit)\n  LL <- exp(fit - 1.96 * se.fit)\n  UL <- exp(fit + 1.96 * se.fit)\n})library(ggplot2)\n\nggplot(new_data, aes(immunoscore, overexpressed_proteins)) +\n    geom_ribbon(aes(ymin = LL, ymax = UL, fill = tumor_subtype), alpha = 0.2) +\n    geom_line(aes(colour = tumor_subtype), size = 1.5) +\n    labs(x = \"Immunoscore\",\n         y = \"Overexpressed Proteins\") +\n    facet_wrap(~ gender)#> Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n#> ℹ Please use `linewidth` instead.\n#> This warning is displayed once every 8 hours.\n#> Call `lifecycle::last_lifecycle_warnings()` to see where this warning was\n#> generated."},{"path":"simSol.html","id":"simSol","chapter":"A Solution: Simulations","heading":"A Solution: Simulations","text":"","code":""},{"path":"simSol.html","id":"sim-sol-ex1","chapter":"A Solution: Simulations","heading":"A.1 Solution Simulation - replacement","text":"option replace=TRUE activates sampling replacement (.e. numbers picked put back can picked ).option replace=FALSE activates sampling without replacement (.e. numbers picked put back picked ).Let’s try :","code":"x <- c( 1, 2, 2, 3, 4, 1, 6, 7, 8, 10, 5, 5, 1, 4, 9 )\n\n# Working example\nsample(x, 10, replace = FALSE)#>  [1] 1 1 3 2 9 1 5 4 5 8# This will cause an error\nsample(x, 20, replace = FALSE)#> Error in sample.int(length(x), size, replace, prob): cannot take a sample larger than the population when 'replace = FALSE'# Fix it\nsample(x, 20, replace = TRUE)#>  [1]  1  5  8  5  7  5 10  1  3  8  6  4  2  7 10  1  3  1\n#> [19]  4  1"},{"path":"simSol.html","id":"sim-sol-ex2","chapter":"A Solution: Simulations","heading":"A.2 Solution Simulation - using sample","text":"gives uniform distribution numbers 1-6. function sample.int specialised version sample sampling integers. Many R libraries specialised versions general functions specific tasks certain conditions.","code":"rolls_from_sample = sample(c(1:6), size=5000, replace=TRUE)\nrolls_from_sample.int = sample(6, size=5000, replace=TRUE)\n\ntable(rolls_from_sample)#> rolls_from_sample\n#>   1   2   3   4   5   6 \n#> 814 815 882 808 847 834table(rolls_from_sample.int)#> rolls_from_sample.int\n#>   1   2   3   4   5   6 \n#> 818 870 818 838 864 792"},{"path":"monopolySol.html","id":"monopolySol","chapter":"B Solutions: Monopoly","heading":"B Solutions: Monopoly","text":"","code":""},{"path":"monopolySol.html","id":"id_3doubleMon","chapter":"B Solutions: Monopoly","heading":"B.1 Solution Exercise 1: Rolling three doubles","text":"can also go jail, roll three doubles (dice value) row. Update code allow possibility going Jail three doubles. distribution board positions change?best process add new feature existing code. way can compare results without new feature.Adding rolling doubles feature doesn’t seem change much. might expect since rolling three doubles unlikely event!","code":"num_turns <- 100000 # number of turns to take\n\ncurrent_board_position <- 0 # start on the GO space\ngo_to_jail_position  <- 30 # the go to jail space\njail_position <- 10 # jail space\n\nmove_size <- rep(0, num_turns)\npositions_visited <- rep(0, num_turns)\n\n# use a for loop to simulate a number of turns\nfor (turn in 1:num_turns) {\n\n  # set double counter to zero\n  double_counter <- 0\n\n  # roll (max) three times\n  for (j in 1:3){\n\n    # roll two dice\n    die_values <- sample(c(1:6), 2, replace = TRUE)\n\n    # if we have rolled a double for the third time, we proceed straight to jail\n    if ((die_values[1] == die_values[2]) & (double_counter == 2 )) {\n      current_board_position <- jail_position\n      break\n    }\n\n    # otherwise\n\n    # move player position\n\n    # number of positions to move\n    plus_move <- sum(die_values)\n\n    # compute new board position\n    new_board_position <- current_board_position + plus_move\n\n    # if land on GO TO JAIL square, then go backwards to the JAIL square\n    if (new_board_position == go_to_jail_position) {\n      new_board_position <- jail_position\n    }\n\n    # update board position (this corrects for the fact the board is circular)\n    current_board_position <- (new_board_position %% 40)\n\n    # break out of loop if we roll a non-double\n    if (die_values[1] != die_values[2]) {\n      break\n    } else { # increment double counter\n      double_counter <- double_counter + 1\n    }\n\n  }\n\n  # store final position visited\n  positions_visited[turn] <- current_board_position\n\n\n}\n\n\nhist(positions_visited, breaks = seq(0, 40, len = 41), right = FALSE)"},{"path":"monopolySol.html","id":"monopolyExtSol","chapter":"B Solutions: Monopoly","heading":"B.2 Solution Exercise 2: Monopoly Extension","text":"example, following simple extension previous example adds features record properties purchased. simulation constructed based assumption players always buys free property land .","code":"num_games <- 1000 # number of games to play\nnum_turns <- 1000 # number of turns to take\n\ncurrent_board_position <- 0 # start on the GO space\ngo_to_jail_position <- 30 # the go to jail space\njail_position <- 10 # jail space\n# vector of squares containing properties\nproperties_that_can_be_bought <- c(1, 3, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16,\n   18, 19, 21, 23, 24, 25, 26, 27, 28, 29, 31, 32, 34, 35, 37, 39)\n\n\n# vector to store number of turns to buy all properties\ntime_to_buy_all_properties <- rep(0, num_games)\n\n# simulate multiple games\nfor (game in 1:num_games) {\n\n  positions_visited <- rep(0, num_turns)\n  positions_purchased <- rep(0, 40)\n  properties_bought <- rep(0, num_turns)\n\n  # use a for loop to simulate a number of turns\n  for (turn in 1:num_turns) {\n\n    # roll two dice\n    die_values <- sample(c(1:6), 2, replace = TRUE)\n\n    # move player position\n\n    # number of positions to move\n    plus_move <- sum(die_values)\n\n    # compute new board position\n    new_board_position <- current_board_position + plus_move\n\n    # if land on GO TO JAIL square, then go backwards to the JAIL square\n    if (new_board_position == go_to_jail_position) {\n      new_board_position <- jail_position\n    }\n\n    # update board position (this corrects for the fact the board is circular)\n    current_board_position <- (new_board_position %% 40)\n\n    # if we can on a square that can be purchased and which has not been\n    # purchased (note R uses 1-indexing for arrays)\n    if (positions_purchased[current_board_position+1] == 0) {\n      if (current_board_position %in% properties_that_can_be_bought) {\n        positions_purchased[current_board_position + 1] <- 1\n      }\n    }\n\n    # store position visited\n    positions_visited[turn] <- current_board_position\n\n    # store number of properties bought\n    properties_bought[turn] <- sum(positions_purchased)\n\n    # check if all properties are gone\n    if (properties_bought[turn] == length(properties_that_can_be_bought)) {\n      time_to_buy_all_properties[game] <- turn\n      break\n    }\n\n\n  }\n\n}\n\nhist(time_to_buy_all_properties, breaks = 20)"},{"path":"mc-solution.html","id":"mc-solution","chapter":"C Solution: 3 state Markov Chain","heading":"C Solution: 3 state Markov Chain","text":"Set 3x3 transition matrix:Note ordering states arbitrary used convention State 1 Sunny, State 2 Rainy State 3 Cloudy means probabilities completed order transition matrix. just need consistent.","code":"transitionMatrix = matrix(c(0.7, 0.2, 0.1,\n                            0.3, 0.3, 0.4,\n                            0.6, 0.2, 0.2), nrow=3, ncol=3, byrow=TRUE)\n\n# Check matrix set-up correctly\nprint(transitionMatrix)#>      [,1] [,2] [,3]\n#> [1,]  0.7  0.2  0.1\n#> [2,]  0.3  0.3  0.4\n#> [3,]  0.6  0.2  0.2state <- 1 # initial state - it is [1] sunny, [2] rainy and [3] cloudy\nweather_sequence <- rep(0, 30) # vector to store simulated values\n\n# simulate for 30 days\nfor (day in 1:30) {\n  pr <- transitionMatrix[state, ] # select the row of transition probabilities\n\n  # sample [1-3] based on the probs pr\n  state <- sample(c(1, 2, 3), size = 1, prob = pr)\n  weather_sequence[day] <- state # store the sampled state\n}\nprint(weather_sequence)#>  [1] 3 2 3 1 2 1 2 1 1 2 3 2 3 1 1 2 1 1 1 2 3 1 1 1 1 1 1 3\n#> [29] 1 1"},{"path":"model-answers-computational-testing.html","id":"model-answers-computational-testing","chapter":"D Model Answers: Computational Testing","heading":"D Model Answers: Computational Testing","text":"","code":""},{"path":"model-answers-computational-testing.html","id":"ht-sol-ex1","chapter":"D Model Answers: Computational Testing","heading":"D.1 Solution: Exercise 1","text":"data :Now construct summary statistics define given parameters:Construct z-statistic:Check z-statistic critical range. First, work z-value edge critical region :Thus, z-statistic much greater threshold evidence suggest cartons overfilled.","code":"x <- c(263.9, 266.2, 266.3, 266.8, 265.0)x_bar <- mean(x) # compute sample mean\nsigma <- 1.65 # population standard deviation is given\nmu <- 260 # population mean to be tested against\nn <- length(x) # number of samplesz <- (x_bar - mu) / (sigma / sqrt(n))\nprint(z)#> [1] 7.643287z_threshold <- qnorm(1 - 0.01, mean = 0, sd = 1)\nprint(z_threshold)#> [1] 2.326348"},{"path":"model-answers-computational-testing.html","id":"ht-sol-ex2","chapter":"D Model Answers: Computational Testing","heading":"D.2 Solution: Exercise 2","text":"Parameters given problem:Compute z-statistic assuming large sample assumptions apply:Now, work thresholds critical regions:z-statistic outside critical regions therefore reject null hypothesis.","code":"x_bar <- 103.11\ns <- 53.5\nmu <- 100\nn <- 45z <- ( x_bar - mu )/(s/sqrt(n))\nprint(z)#> [1] 0.3899535z_upper <- qnorm(1 - 0.025, mean = 0, sd = 1)\nprint(z_upper)#> [1] 1.959964z_lower <- qnorm(0.025, mean = 0, sd = 1)\nprint(z_lower)#> [1] -1.959964"},{"path":"model-answers-computational-testing.html","id":"ht-sol-ex3","chapter":"D Model Answers: Computational Testing","heading":"D.3 Solution: Exercise 3","text":"","code":"z_test <- function(x, mu, popvar){\n\n  one_tail_p <- NULL\n\n  z_score <- round((mean(x) - mu) / (popvar / sqrt(length(x))), 3)\n\n  one_tail_p <- round(pnorm(abs(z_score),lower.tail = FALSE), 3)\n\n  cat(\" z =\", z_score, \"\\n\",\n    \"one-tailed probability =\", one_tail_p, \"\\n\",\n    \"two-tailed probability =\", 2 * one_tail_p)\n\n  return(list(z = z_score, one_p = one_tail_p, two_p = 2 * one_tail_p))\n}\n\nx <- rnorm(10, mean = 0, sd = 1) # generate some artificial data from a N(0, 1)\nout <- z_test(x, 0, 1) # null should not be rejected!#>  z = -0.766 \n#>  one-tailed probability = 0.222 \n#>  two-tailed probability = 0.444print(out)#> $z\n#> [1] -0.766\n#> \n#> $one_p\n#> [1] 0.222\n#> \n#> $two_p\n#> [1] 0.444x <- rnorm(10, mean = 1, sd = 1) # generate some artificial data from a N(1, 1)\nout <- z_test(x, 0, 1) # null should be rejected!#>  z = 3.062 \n#>  one-tailed probability = 0.001 \n#>  two-tailed probability = 0.002print(out)#> $z\n#> [1] 3.062\n#> \n#> $one_p\n#> [1] 0.001\n#> \n#> $two_p\n#> [1] 0.002"},{"path":"model-answers-computational-testing.html","id":"ht-sol-ex4","chapter":"D Model Answers: Computational Testing","heading":"D.4 Solution: Exercise 4","text":"Define parametersCompute t-statistic:Work thresholds critical regions:t-statistic outside critical regions reject null hypothesis.","code":"mu <- 5.4\nn <- 5\nx_bar <- 5.64\ns2 <- 0.05t <- (x_bar - mu) / sqrt(s2 / n)\nprint(t)#> [1] 2.4t_upper <- qt(1 - 0.025, df = n - 1)\nprint(t_upper)#> [1] 2.776445t_lower <- qt(0.025, df = n - 1)\nprint(t_lower)#> [1] -2.776445"},{"path":"model-answers-computational-testing.html","id":"ht-sol-ex5","chapter":"D Model Answers: Computational Testing","heading":"D.5 Solution: Exercise 5","text":"Define parameters:Compute z-statistic:Work 5% significance level, critical values:evidence support claim process \\(\\) yields higher pressurisation.","code":"x_bar_a <- 88\ns2_a <- 4.5\nn_a <- 72\nx_bar_b <- 79\ns2_b <- 4.2\nn_b <- 48\nmu_a <- 0\nmu_b <- 0z <- ((x_bar_a - x_bar_b) - (mu_a - mu_b)) / sqrt(s2_a / n_a + s2_b / n_b)\nprint(z)#> [1] 23.2379z_upper <- qnorm(1 - 0.05, mean = 0, sd = 1)\nprint(z_upper)#> [1] 1.644854"},{"path":"model-answers-computational-testing.html","id":"ht-sol-ex6","chapter":"D Model Answers: Computational Testing","heading":"D.6 Solution: Exercise 6","text":"Compute pooled variance estimator:Compute t-statistic:Work critical values:Since \\(|t|<2.14\\) evidence reject null hypothesis mean yields equal.Now, let us use built-t.test command:options paired=FALSE means unpaired t-test, var.equal=TRUE forces estimated variances (.e. using pooled variance estimator) testing 95% confidence level alternative hypothesis true difference means non-zero.p-value t-test 0 1. case, value around 0.72 means hypothesis reject.","code":"# Data vectors\nx_A <- c(91.50, 94.18, 92.18, 95.39, 91.79, 89.07, 94.72, 89.21)\nx_B <- c(89.19, 90.95, 90.46, 93.21, 97.19, 97.04, 91.07, 92.75)\n\n# parameters based on data\nx_bar_A <- mean(x_A)\ns2_A <- var(x_A)\nn_A <- length(x_A)\nx_bar_B <- mean(x_B)\ns2_B <- var(x_B)\nn_B <- length(x_B)s2_p <- ((n_A - 1) * s2_A + (n_B - 1) * s2_B) / (n_A + n_B - 2)\nprint(s2_p)#> [1] 7.294654t = ( x_bar_A - x_bar_B ) / sqrt( s2_p*(1/n_A + 1/n_B) )\nprint(t)#> [1] -0.3535909t_upper <- qt(1 - 0.025, df = n_A + n_B - 2)\nprint(t_upper)#> [1] 2.144787t_lower <- qt(0.025, df = n_A + n_B - 2)\nprint(t_lower)#> [1] -2.144787  out <- t.test(x = x_A, y = x_B, paired = FALSE, var.equal = TRUE,\n    conf.level = 0.95, mu = 0, alternative = \"two.sided\")\n  print(out)#> \n#>  Two Sample t-test\n#> \n#> data:  x_A and x_B\n#> t = -0.35359, df = 14, p-value = 0.7289\n#> alternative hypothesis: true difference in means is not equal to 0\n#> 95 percent confidence interval:\n#>  -3.373886  2.418886\n#> sample estimates:\n#> mean of x mean of y \n#>   92.2550   92.7325"},{"path":"model-answers-computational-testing.html","id":"ht-sol-ex7","chapter":"D Model Answers: Computational Testing","heading":"D.7 Solution: Exercise 7","text":"Define parameters:Compute expected counts:Compute chi-squared statistic:Compute critical value form chi-squared distribution:Thus evidence reject null hypothesis. data provides reason suggest preference particular door.Now, done R:","code":"x <- c(23, 36, 31)\np <- c(1 / 3, 1 / 3, 1 / 3)\nn <- sum(x)\nK <- length(x)Ex = p*nchi2 <- sum((x - Ex)^2 / Ex)\nprint(chi2)#> [1] 2.866667chi_upper <- qchisq(1 - 0.05, df = K-1)\nprint(chi_upper)#> [1] 5.991465out <- chisq.test(x, p = c(1 / 3, 1 / 3, 1 / 3))\nprint(out)#> \n#>  Chi-squared test for given probabilities\n#> \n#> data:  x\n#> X-squared = 2.8667, df = 2, p-value = 0.2385"},{"path":"model-answers-computational-testing.html","id":"ht-sol-ex8","chapter":"D Model Answers: Computational Testing","heading":"D.8 Solution: Exercise 8","text":"need vcdExtra package use expand.dft command. using BearPortal don’t need install , running practical computer need install package install.packages(\"vcdExtra\").Now can use expand.dft() function need load package containing function. expand.dft command allows one convert frequency table vector samples:Now can use fitdistr function MASS package estimate MLE Poisson distributionLet just solve directly using R built function. First compute expected probabilities Poisson distribution using dpois compute Poisson pdf:apply chisq.test:Actually, case answer wrong(!), need apply additional loss degree freedom account use MLE. However, can re-use values already computed chisq.test:Hence, evidence reject null hypothesis. reason suppose Poisson distribution plausible model number accidents per week junction.","code":"y <- c( 0, 1, 2 )\nx <- c( 32, 12, 6 )library(vcdExtra)#> Loading required package: vcd#> Warning: package 'vcd' was built under R version 4.3.3#> Loading required package: grid#> Loading required package: gnmsamples <- expand.dft(data.frame(y,Frequency = x), freq = \"Frequency\")# loading the MASS package\nlibrary(MASS)\n\n# fitting a Poisson distribution using maximum-likelihood\nlambda_hat <- fitdistr(samples$y, densfun = 'Poisson')pr <- c(0, 0, 0)\npr[1] <- dpois(0, lambda = lambda_hat$estimate)\npr[2] <- dpois(1, lambda = lambda_hat$estimate)\npr[3] <- 1 - sum(pr[1:2])out <- chisq.test(x, p = pr)#> Warning in chisq.test(x, p = pr): Chi-squared approximation\n#> may be incorrectprint(out)#> \n#>  Chi-squared test for given probabilities\n#> \n#> data:  x\n#> X-squared = 1.3447, df = 2, p-value = 0.5105chi2 <- out$statistic\nprint(chi2)#> X-squared \n#>   1.34466chi2_lower <- qchisq(1 - 0.01, df = 1)\nprint(chi2_lower)#> [1] 6.634897"},{"path":"solutions-linear-regression-and-correlation.html","id":"solutions-linear-regression-and-correlation","chapter":"E Solutions: Linear regression and Correlation","heading":"E Solutions: Linear regression and Correlation","text":"","code":""},{"path":"solutions-linear-regression-and-correlation.html","id":"LR-sol1","chapter":"E Solutions: Linear regression and Correlation","heading":"Solution Exercise I: Correlation","text":"Read data stork.txt, compute correlation comment .data represents storks (column 1) Oldenburg Germany \\(1930 - 1939\\) number people (column 2).plot number people Oldenburg (Germany) number storks. can calculate correlation RThis high correlation, obviously causation. Think correlation two random variables.","code":"library(ggplot2)\nlibrary(reshape2)stork_dat <- read.table(\"stork.txt\", hedaer = TRUE)ggplot(stork_dat, aes(x = no_storks, y = people)) +\n  geom_point(size = 2)cor(stork_dat$no_storks, stork_dat$peopl)#> [1] 0.9443965"},{"path":"solutions-linear-regression-and-correlation.html","id":"LR-sol2","chapter":"E Solutions: Linear regression and Correlation","heading":"Solution Exercise II","text":"","code":"# load first data set and create data.frame\nload(\"lr_data1.Rdata\")\nsim_data1 <- data.frame(x = x, y = y)\n\n# load second data set and create data.frame\nload(\"lr_data2.Rdata\")\nsim_data2 <- data.frame(x = x, y = y)\n\nlr_fit1 <- lm(y ~ x, data = sim_data1)\nlr_fit2 <- lm(y ~ x, data = sim_data2)"},{"path":"solutions-linear-regression-and-correlation.html","id":"comparison-of-data","chapter":"E Solutions: Linear regression and Correlation","heading":"E.0.1 Comparison of data","text":"plot data top , first data set black second one red, can see small number points different two data sets.summary data can see discrepancy two estimates regression coefficients (\\(\\approx 1\\)), though error estimate quite large. thing notice summary residuals look quite different. investigate plot see:can see outliers second data set affect estimation. now plot histogram boxplots comparison:can see distribution residuals significantly changed data set 2.change 4 data points sufficient change regression coefficients.","code":"ggplot(sim_data1, aes(x = x, y = y)) +\n  geom_point(size = 1.5) +\n  geom_point(data = sim_data2, color = \"red\", shape = 18)summary(lr_fit1)#> \n#> Call:\n#> lm(formula = y ~ x, data = sim_data1)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -2.8309 -0.6910  0.0296  0.7559  3.3703 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) -20.1876     0.1250 -161.46   <2e-16 ***\n#> x             2.8426     0.1138   24.98   <2e-16 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 1.229 on 98 degrees of freedom\n#> Multiple R-squared:  0.8643, Adjusted R-squared:  0.8629 \n#> F-statistic: 624.2 on 1 and 98 DF,  p-value: < 2.2e-16summary(lr_fit2)#> \n#> Call:\n#> lm(formula = y ~ x, data = sim_data2)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -11.386  -1.960  -1.084  -0.206  54.516 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) -18.9486     0.8006 -23.669  < 2e-16 ***\n#> x             2.1620     0.7285   2.968  0.00377 ** \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 7.87 on 98 degrees of freedom\n#> Multiple R-squared:  0.08245,    Adjusted R-squared:  0.07309 \n#> F-statistic: 8.806 on 1 and 98 DF,  p-value: 0.003772plot(residuals(lr_fit1))plot(residuals(lr_fit2))hist(residuals(lr_fit1))hist(residuals(lr_fit2))boxplot(residuals(lr_fit2), residuals(lr_fit1))"},{"path":"solutions-linear-regression-and-correlation.html","id":"LR-sol3","chapter":"E Solutions: Linear regression and Correlation","heading":"Solution Exercise II","text":"see variance mean-squared error goes sample size goes converges towards limiting value. Larger sample sizes help reduce variance estimators make estimates accurate.Can something similar work relationship accurate regression coefficient estimates function sample size?","code":"b0 <- 10 # regression coefficient for intercept\nb1 <- -8 # regression coefficient for slope\nsigma2 <- 0.5 # noise variance\n\n# number of simulations for each sample size\nn_simulations <- 100\n\n# A vector of sample sizes to try\nsample_size_v <- c( 5, 20, 40, 80, 100, 150, 200, 300, 500, 750, 1000 )\n\nn_sample_size <- length(sample_size_v)\n\n# Create a matrix to store results\nmse_matrix <- matrix(0, nrow = n_simulations, ncol = n_sample_size)\n\n# name row and column\nrownames(mse_matrix) <- c(1:n_simulations)\ncolnames(mse_matrix) <- sample_size_v# loop over sample size\nfor (i in 1:n_sample_size) {\n  N <- sample_size_v[i]\n\n  # for each simulation\n  for (it in 1:n_simulations) {\n\n    x <- rnorm(N, mean = 0, sd = 1)\n    e <- rnorm(N, mean = 0, sd = sqrt(sigma2))\n    y <- b0 + b1 * x + e\n\n    # set up a data frame and run lm()\n    sim_data <- data.frame(x = x, y = y)\n    lm_fit <- lm(y ~ x, data = sim_data)\n\n    # compute the mean squared error between the fit and the actual y's\n    y_hat <- fitted(lm_fit)\n    mse_matrix[it, i] <- mean((y_hat - y)^2)\n\n  }\n}library(reshape2)\n\nmse_df <- melt(mse_matrix) # convert the matrix into a data frame for ggplot\nnames(mse_df) = c(\"Simulation\", \"Sample_Size\", \"MSE\") # rename the columns\n\n# now use a boxplot to look at the relationship between mean-squared prediction error and sample size\nmse_plt = ggplot(mse_df, aes(x=Sample_Size, y=MSE))\nmse_plt = mse_plt + geom_boxplot( aes(group=Sample_Size) )\nprint(mse_plt)"}]
